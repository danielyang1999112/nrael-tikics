{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ba863345-96e7-4067-91fa-3cfd6abcaf1e",
   "metadata": {},
   "source": [
    "# Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eded3418-80cb-4285-b378-ec5f9d35d29e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import log_loss\n",
    "\n",
    "def linear(x):\n",
    "    return x\n",
    "\n",
    "def sigmoid( x):\n",
    "    return 1/(1 + np.exp(-x))\n",
    "\n",
    "class NeuralNetwork:\n",
    "    \n",
    "    def __init__(self, lr, max_iter, random_state=None):\n",
    "        self.lr = lr\n",
    "        self.max_iter = max_iter\n",
    "        self.random_state = random_state\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        y = y.reshape(-1, 1)\n",
    "        # Set random seed if self.random_state is not none\n",
    "        if self.random_state is not None:\n",
    "            np.random.seed(self.random_state)\n",
    "\n",
    "        self.W1 = np.random.rand(30,10)\n",
    "        self.b1 = np.random.rand(10)\n",
    "        self.W2 = np.random.rand(10,1)\n",
    "        self.b2 = np.random.rand(1)\n",
    "        \n",
    "        loss_history = []\n",
    "        \n",
    "        for i in range(self.max_iter):\n",
    "            # ==== Forward ====\n",
    "            Z2 = X @ self.W1 + self.b1\n",
    "            A2 = linear(Z2)\n",
    "\n",
    "            Z3 = A2 @ self.W2 + self.b2\n",
    "            y_pred = sigmoid(Z3) # A3\n",
    "            \n",
    "            # Calculate loss/error\n",
    "            loss = log_loss(y, y_pred)\n",
    "            loss_history.append(loss)\n",
    "            \n",
    "            # ==== Backward ====\n",
    "            # Element-wise loss required for partial d's\n",
    "            e = (y_pred - y) # Delta_3\n",
    "            \n",
    "            # Output layer partial d's\n",
    "            dW2 = A2.T @ e\n",
    "            db2 = np.sum(e)\n",
    "            \n",
    "            # Hidden layer partial d's\n",
    "            dW1 = X.T @ (e @ self.W2.T)\n",
    "            db1 = np.sum(e @ self.W2.T, axis=0)\n",
    "\n",
    "            # Update weights\n",
    "            self.W1 = self.W1 - self.lr * dW1\n",
    "            self.b1 = self.b1 - self.lr * db1\n",
    "            self.W2 = self.W2 - self.lr * dW2\n",
    "            self.b2 = self.b2 - self.lr * db2\n",
    "\n",
    "        return loss_history\n",
    "\n",
    "    def predict(self, X):\n",
    "        y_pred = (self.predict_proba(X) > 0.5).astype(int)\n",
    "        return y_pred\n",
    "    \n",
    "    def predict_proba(self, X):\n",
    "        Z2 = X @ self.W1 + self.b1\n",
    "        a2 = linear(Z2)\n",
    "        \n",
    "        Z3 = a2 @ self.W2 + self.b2\n",
    "        A3 = sigmoid(Z3)\n",
    "\n",
    "        return A3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12296942-5273-4f9d-9147-8710ef387c93",
   "metadata": {},
   "source": [
    "# Inspect loss history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f84e5397-9329-4351-8918-e1be4877a331",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import log_loss\n",
    "\n",
    "# COPY YOUR CLASS DEFINITION HERE  \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import log_loss\n",
    "\n",
    "def linear(x):\n",
    "    return x\n",
    "\n",
    "def sigmoid( x):\n",
    "    return 1/(1 + np.exp(-x))\n",
    "\n",
    "class NeuralNetwork:\n",
    "    \n",
    "    def __init__(self, lr, max_iter, random_state=None):\n",
    "        self.lr = lr\n",
    "        self.max_iter = max_iter\n",
    "        self.random_state = random_state\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        y = y.reshape(-1, 1)\n",
    "        # Set random seed if self.random_state is not none\n",
    "        if self.random_state is not None:\n",
    "            np.random.seed(self.random_state)\n",
    "\n",
    "        self.W1 = np.random.rand(30,10)\n",
    "        self.b1 = np.random.rand(10)\n",
    "        self.W2 = np.random.rand(10,1)\n",
    "        self.b2 = np.random.rand(1)\n",
    "        \n",
    "        loss_history = []\n",
    "        \n",
    "        for i in range(self.max_iter):\n",
    "            # ==== Forward ====\n",
    "            Z2 = X @ self.W1 + self.b1\n",
    "            A2 = linear(Z2)\n",
    "\n",
    "            Z3 = A2 @ self.W2 + self.b2\n",
    "            y_pred = sigmoid(Z3) # A3\n",
    "            \n",
    "            # Calculate loss/error\n",
    "            loss = log_loss(y, y_pred)\n",
    "            loss_history.append(loss)\n",
    "            \n",
    "            # ==== Backward ====\n",
    "            # Element-wise loss required for partial d's\n",
    "            e = (y_pred - y) # Delta_3\n",
    "            \n",
    "            # Output layer partial d's\n",
    "            dW2 = A2.T @ e\n",
    "            db2 = np.sum(e)\n",
    "            \n",
    "            # Hidden layer partial d's\n",
    "            dW1 = X.T @ (e @ self.W2.T)\n",
    "            db1 = np.sum(e @ self.W2.T, axis=0)\n",
    "\n",
    "            # Update weights\n",
    "            self.W1 = self.W1 - self.lr * dW1\n",
    "            self.b1 = self.b1 - self.lr * db1\n",
    "            self.W2 = self.W2 - self.lr * dW2\n",
    "            self.b2 = self.b2 - self.lr * db2\n",
    "\n",
    "        return loss_history\n",
    "\n",
    "    def predict(self, X):\n",
    "        y_pred = (self.predict_proba(X) > 0.5).astype(int)\n",
    "        return y_pred\n",
    "    \n",
    "    def predict_proba(self, X):\n",
    "        Z2 = X @ self.W1 + self.b1\n",
    "        a2 = linear(Z2)\n",
    "        \n",
    "        Z3 = a2 @ self.W2 + self.b2\n",
    "        A3 = sigmoid(Z3)\n",
    "\n",
    "        return A3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a8209009-aca8-42c4-9288-e06fc01d81fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# LOAD \n",
    "data = pd.read_csv('data/train_w4.csv') \n",
    "\n",
    "X = data.iloc[:,:-1].to_numpy()\n",
    "y = data.iloc[:, -1].to_numpy()\n",
    "\n",
    "network = NeuralNetwork(0.0001, 1000, 6850)\n",
    "\n",
    "loss_list = network.fit(X, y)\n",
    "\n",
    "# PLOT THE LOSS HISTORY HERE\n",
    "plt.plot(loss_list)\n",
    "\n",
    "plt.xlabel('No. of Iteration')\n",
    "plt.ylabel('Loss')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "981f4edf-d55e-48bb-9eaf-20d2c460f5e0",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e5f8615-4210-4e9c-a137-b370d0438e24",
   "metadata": {},
   "source": [
    "# Pytorch neural network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e205e51d-68fe-4650-a2f2-6178e18ad60d",
   "metadata": {},
   "source": [
    "- ## 1. Define the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0d0f4e56-d7ae-43a3-a628-05650a97c168",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "np.random.seed(2022)   # To make sure the results are reproducable\n",
    "torch.manual_seed(2022)\n",
    "\n",
    "class NeuralNetwork (torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NeuralNetwork, self).__init__()   # Previously we dont have this line. Here we are initialising nn.Module       \n",
    "        self.Identity = torch.nn.Identity()   # Define activations\n",
    "        self.Sigmoid = torch.nn.Sigmoid()\n",
    "        \n",
    "        self.Linear1 = torch.nn.Linear(30, 10) # Define linear\n",
    "        self.Linear2 = torch.nn.Linear(10, 1)\n",
    "        \n",
    "    def forward(self, X):\n",
    "        Z2 = self.Linear1(X)      # first layer\n",
    "        A2 = self.Identity(Z2)     # activation\n",
    "        \n",
    "        Z3 = self.Linear2(A2)      # to second layer\n",
    "        a3 = self.Sigmoid(Z3)      # activation\n",
    "        return a3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77efa718-e52f-493c-88c3-f109d9529b46",
   "metadata": {},
   "source": [
    "- ## 2. Initialize a Network (create an instance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c51077c8-f6d4-49f2-aead-fa8da4bce466",
   "metadata": {},
   "outputs": [],
   "source": [
    "network = NeuralNetwork() "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53b60e18-16ac-4a0d-872a-c25208b101a3",
   "metadata": {},
   "source": [
    "- ## 3. Prepare Data for Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3cc473bd-f8fb-46fd-aa76-a5448a067bc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('data/train_w4.csv') \n",
    "\n",
    "X = torch.tensor(data.iloc[:,:-1].to_numpy()).float()\n",
    "y = torch.tensor(data.iloc[:, -1].to_numpy()).float()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a4ba390-45e9-4695-9d4a-fa01d9c31021",
   "metadata": {},
   "source": [
    "- ## 4. Define Objective Function and Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fc8729bf-b296-4d7e-9f62-c28650308568",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_func = torch.nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(network.parameters(), lr=0.0001)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa57bbee-73dd-45f0-a4d0-7ca8cbdf76a2",
   "metadata": {},
   "source": [
    "- ## 5. Train the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "93b18b9a-f230-4abe-a2c1-5cb70314b8b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss in iteration 0: 0.8831\n",
      "Loss in iteration 1000: 0.2128\n",
      "Loss in iteration 2000: 0.1110\n",
      "Loss in iteration 3000: 0.0812\n",
      "Loss in iteration 4000: 0.0682\n",
      "Loss in iteration 5000: 0.0610\n",
      "Loss in iteration 6000: 0.0561\n",
      "Loss in iteration 7000: 0.0520\n",
      "Loss in iteration 8000: 0.0477\n",
      "Loss in iteration 9000: 0.0434\n",
      "Loss in iteration 10000: 0.0393\n",
      "Loss in iteration 11000: 0.0356\n",
      "Loss in iteration 12000: 0.0323\n",
      "Loss in iteration 13000: 0.0292\n",
      "Loss in iteration 14000: 0.0261\n",
      "Loss in iteration 15000: 0.0233\n",
      "Loss in iteration 16000: 0.0208\n",
      "Loss in iteration 17000: 0.0185\n",
      "Loss in iteration 18000: 0.0164\n",
      "Loss in iteration 19000: 0.0145\n"
     ]
    }
   ],
   "source": [
    "train_loss = []\n",
    "iters = 20000\n",
    "for i in range(iters):\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    y_pred = network(X)\n",
    "    loss = loss_func(y_pred, y.view(-1,1))  # We must make both y_pred and y in the exact same shape\n",
    "    \n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    train_loss.append(loss.item()) \n",
    "\n",
    "    if i % 1000 == 0:\n",
    "        print('Loss in iteration {}: {:.4f}'.format(i, loss.item()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ac886e0-cb7b-4dc0-ae1a-81de1f06d49c",
   "metadata": {},
   "source": [
    "- ## 6. Plot the loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4aeb92f3-7126-41e9-90e4-4a12f5cefbe2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEGCAYAAABo25JHAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/YYfK9AAAACXBIWXMAAAsTAAALEwEAmpwYAAAegElEQVR4nO3de5hcdZ3n8fe3qrqqq/qaS0Mu5IYmSBQQjBEX4zDLoMA44mWeEXBHZcZh2BUdx2dnZdZ9fNx1/tB1nMdxvbCMMowzIsx6ZQXFfVRAZdAECJcAITEECLl1bt2dvlf1d/84pzqnK92d7nSdqu4+n9fz9FN1LnXqm1Od+vTv/M75HXN3REQkuVL1LkBEROpLQSAiknAKAhGRhFMQiIgknIJARCThMvUuYLoWL17sq1evrncZIiJzyiOPPHLI3TvGWzbngmD16tVs2bKl3mWIiMwpZvbCRMt0aEhEJOEUBCIiCacgEBFJOAWBiEjCKQhERBJOQSAiknAKAhGRhEtMEGzf38Pf3redw8cH612KiMiskpgg2NV5nC/9fCcHuhUEIiJRiQmCfDYNQP9wsc6ViIjMLokJgkI2GE2jb6hU50pERGaXBAVB0CJQEIiIjJW4IOhXEIiIjJGgINChIRGR8SQmCPKjh4bUWSwiEpWYIFAfgYjI+BITBA3pFA1pUxCIiFRITBAA5BvS9OvQkIjIGIkKgkI2oxaBiEiFZAVBLk3fsIJARCQqWUGQTes6AhGRCskKgoYMvYPqIxARiUpUEOSzafp1aEhEZIxEBUEhm1ZnsYhIhViDwMyuMLPtZrbTzG4eZ3mbmf1fM3vczLaZ2fVx1pNXH4GIyEliCwIzSwNfBq4E1gPXmtn6itU+BDzt7hcAlwKfN7NsXDU1ZTMaYkJEpEKcLYKNwE533+XuQ8CdwNUV6zjQYmYGNANHgNi+qXVoSETkZHEGwXLgpcj0nnBe1JeAc4G9wJPAX7j7SOWGzOwGM9tiZls6OztPu6B8Ns1gcYTSiJ/2NkRE5ps4g8DGmVf5DfxWYCuwDHgt8CUzaz3pRe63uvsGd9/Q0dFx2gUVNAKpiMhJ4gyCPcCKyPRZBH/5R10PfNcDO4HngVfFVVA+vCeBOoxFRE6IMwg2A2vNbE3YAXwNcHfFOi8ClwGY2ZnAOcCuuAoqNGgoahGRSpm4NuzuRTO7CbgPSAO3ufs2M7sxXH4L8GngdjN7kuBQ0sfd/VBcNemeBCIiJ4stCADc/V7g3op5t0Se7wXeEmcNUYVceGhoWH0EIiJlibuyGNQiEBGJSlQQ5MM+gt5BBYGISFmigqDcItChIRGRExIWBEEfgQ4NiYickKggyJdbBAoCEZFRiQqCpqz6CEREKiUqCDLpFI0NKY4PDte7FBGRWSNRQQDQnGvguFoEIiKjEhgEaY7rvsUiIqOSFwSNuoG9iEhU4oKgKZvh+ICCQESkLHFB0NKY0aEhEZGIxAVBUy5Dr25MIyIyKnFB0JzToSERkahkBoEODYmIjEpkEAwWRxgujdS7FBGRWSFxQdAU3pxGp5CKiAQSFwTNjUEQ9KifQEQESGIQlFsEOnNIRARIYBDo0JCIyFiJC4Jyi0CHhkREAokNAt2TQEQkkLwgCDuLdU8CEZFA8oIgWw4CtQhERCCBQdCUC25XqWEmREQCiQuC8u0qdfqoiEggcUEA5dtVKghERCCxQZDWoSERkVAyg0C3qxQRGZXIIGjKZnRBmYhIKJFB0JpvoHtA1xGIiEBSg6CxQS0CEZFQMoMgn6G7Xy0CERFIaBC05RvoGSxSGvF6lyIiUneJDILWxgYAetRPICKS0CDIB0HQ3a9+AhGRWIPAzK4ws+1mttPMbp5gnUvNbKuZbTOzB+Ksp6w1HIFUZw6JiEAmrg2bWRr4MnA5sAfYbGZ3u/vTkXXaga8AV7j7i2Z2Rlz1RJ1oESgIRETibBFsBHa6+y53HwLuBK6uWOc64Lvu/iKAux+MsZ5RbeUgUItARCTWIFgOvBSZ3hPOi1oHLDCz+83sETN7X4z1jFIfgYjICbEdGgJsnHmV52tmgNcBlwF54N/M7GF3f27MhsxuAG4AWLly5YwLK/cRdOnQkIhIrC2CPcCKyPRZwN5x1vmxu/e6+yHgQeCCyg25+63uvsHdN3R0dMy4sKZshpTp0JCICMQbBJuBtWa2xsyywDXA3RXr/ADYZGYZMysAbwCeibEmAFIpo6WxQZ3FIiLEeGjI3YtmdhNwH5AGbnP3bWZ2Y7j8Fnd/xsx+DDwBjABfc/en4qopqjWfoVvjDYmIxNpHgLvfC9xbMe+WiunPAZ+Ls47xtOXVIhARgYReWQzBMBPqLBYRSXgQqLNYRCTJQZDP6DoCERGSHARqEYiIAAkOgrZ8A31DJYaKI/UuRUSkrhIbBAuasgAc6xuqcyUiIvWV2CBYGAbBEQWBiCRcYoOgvRAMPHekV0EgIsmW2CBYOHpoSB3GIpJsyQ2CQnhoSC0CEUm4xAZBexgERxUEIpJwiQ2CbCZFcy6jzmIRSbzEBgHAgqYG9RGISOIlOwgKWfURiEjiJT4IjurQkIgkXKKDYGGTgkBEJNFB0F5o4Giv+ghEJNkSHQQLC1mODxYZLJbqXYqISN0kOggW6OpiEZFkB0F5mAn1E4hIkiU6CBaEVxcfPq4gEJHkmlIQmFmTmaXC5+vM7O1m1hBvafHraMkBcOj4YJ0rERGpn6m2CB4EGs1sOfBT4Hrg9riKqpVyEHT2KAhEJLmmGgTm7n3Au4D/5e7vBNbHV1ZttDZmyGZSCgIRSbQpB4GZvRF4L3BPOC8TT0m1Y2Z0NOcUBCKSaFMNgo8Cfw18z923mdnZwM9jq6qGFrfk6FQfgYgk2JT+qnf3B4AHAMJO40Pu/pE4C6uVjuYce4721bsMEZG6mepZQ3eYWauZNQFPA9vN7K/iLa02OlpyOmtIRBJtqoeG1rt7N/AO4F5gJfDHcRVVSx0tOQ73DlEsjdS7FBGRuphqEDSE1w28A/iBuw8DHltVNdTRksNd9y4WkeSaahD8b2A30AQ8aGargO64iqqljubgWoKDOnNIRBJqqp3FXwS+GJn1gpn9bjwl1dboRWXqJxCRhJpqZ3Gbmf2dmW0Jfz5P0DqY887Q1cUiknBTPTR0G9AD/FH40w38Y1xF1ZKGmRCRpJvq1cGvcPd3R6b/u5ltjaGemmtsSNNeaGBfV3+9SxERqYuptgj6zexN5QkzuwSYN9+cS9vy7Ds2UO8yRETqYqotghuBb5hZWzh9FHh/PCXV3rK2RvZ2KQhEJJmm1CJw98fd/QLgfOB8d78Q+Penep2ZXWFm281sp5ndPMl6rzezkpn94ZQrr6Kl7Y06NCQiiTWtO5S5e3d4hTHAxyZb18zSwJeBKwmGrL7WzE4aujpc77PAfdOppZqWtuU51jdM/5BuYi8iyTOTW1XaKZZvBHa6+y53HwLuBK4eZ70PA98BDs6glhlZ1t4IwF61CkQkgWYSBKcaYmI58FJkek84b1R4x7N3ArdMtiEzu6F8DUNnZ+fp1DqppW15AHUYi0giTdpZbGY9jP+Fb0D+FNser8VQua0vAB9395LZxA0Md78VuBVgw4YNVR/jaFkYBGoRiEgSTRoE7t4yg23vAVZEps8C9lasswG4MwyBxcBVZlZ09+/P4H2n7cy24KIytQhEJInivN3kZmCtma0BXgauAa6LruDua8rPzex24Ie1DgGAXCbN4uaczhwSkUSKLQjcvWhmNxGcDZQGbgtvc3ljuHzSfoFaW9beyMvHFAQikjyx3oDe3e8luJFNdN64AeDuH4izllNZsbDAtpe76lmCiEhdzOSsoXll1cICe472605lIpI4CoLQqkUFiiPOXnUYi0jCKAhCKxcGt1d44UhvnSsREaktBUFo1aICAC8c7qtzJSIitaUgCC1pbSSbSfHiEQWBiCSLgiCUShkrFuR54bAODYlIsigIIlYtatKhIRFJHAVBxMqFBV480od71YczEhGZtRQEEa84o5m+oRL7u3UKqYgkh4IgYu0ZzQDsOHC8zpWIiNSOgiCiHATPHeipcyUiIrWjIIhY1JxjYVOWnQfVIhCR5FAQVHjlGc3sUBCISIIoCCqsPaOZHQd6dOaQiCSGgqDCujNb6B4o0tkzWO9SRERqQkFQodxhvF0dxiKSEAqCCucubQVg297uOlciIlIbCoIKC5qyLG/P85TuViYiCaEgGMdrlreqRSAiiaEgGMd5y9t4/lAv3QPD9S5FRCR2CoJxvHp5GwBPq1UgIgmgIBjHa5YFQaB+AhFJAgXBODpacixta+TxPQoCEZn/FAQTuGjVAh7ZfaTeZYiIxE5BMIHXr1rA3q4BXj7WX+9SRERipSCYwIbVCwHYolaBiMxzCoIJvGpJC825DJsVBCIyzykIJpBJp7hwZTtbdh+tdykiIrFSEExi4+qFbD/Qw5HeoXqXIiISGwXBJDat68AdfrGjs96liIjERkEwifOWt9FeaOCB5xQEIjJ/KQgmkU4Zm9Z28OBzhxgZ0R3LRGR+UhCcwpvXLubQ8UGe2a9xh0RkflIQnMLvrOsA4GfPHKxzJSIi8VAQnMIZrY1ctLKdHz21v96liIjEQkEwBVedt5Sn93Wz+1BvvUsREam6WIPAzK4ws+1mttPMbh5n+XvN7Inw5yEzuyDOek7XlectBVCrQETmpdiCwMzSwJeBK4H1wLVmtr5iteeB33H384FPA7fGVc9MLG/Pc8GKdu55cm+9SxERqbo4WwQbgZ3uvsvdh4A7gaujK7j7Q+5eHsPhYeCsGOuZkbdfsIynXu7mWZ09JCLzTJxBsBx4KTK9J5w3kT8FfjTeAjO7wcy2mNmWzs76XNz1zguXk02nuGvzS6deWURkDokzCGyceeNelWVmv0sQBB8fb7m73+ruG9x9Q0dHRxVLnLqFTVkuf/WZfO+xlxkslupSg4hIHOIMgj3Aisj0WcBJB9nN7Hzga8DV7n44xnpm7D0bVnCsb5gfq9NYROaROINgM7DWzNaYWRa4Brg7uoKZrQS+C/yxuz8XYy1V8aZXLmbN4ia+/svncdeQEyIyP8QWBO5eBG4C7gOeAf7V3beZ2Y1mdmO42ieBRcBXzGyrmW2Jq55qSKWMD25awxN7unh4l25YIyLzg821v2w3bNjgW7bULy8Ghktc8pmfcd5Zbdx+/ca61SEiMh1m9oi7bxhvma4snqbGhjTXX7Ka+7d38tiLunuZiMx9CoLT8IFL1rCoKctnf/ys+gpEZM5TEJyG5lyGj1y2lod3HdFNa0RkzlMQnKZrN65k1aIC/+OHTzMwrOsKRGTuUhCcpmwmxaevfg27Onv5yv2/rXc5IiKnTUEwA29e18E7XruMr96/U2MQicicpSCYof/2tvW05bN8+I7H6Bsq1rscEZFpUxDM0OLmHF94z2vZ2XmcT929rd7liIhMm4KgCt60djEfuvSV/OuWPdz2y+frXY6IyLRk6l3AfPGXl69jx8EePn3P0yxfkOetr15S75JERKZELYIqSaeML7znQi44q50Pf+sxfr79YL1LEhGZEgVBFeWzaf7xA69n7RnN/Pk3HuEn2zRctYjMfgqCKlvQlOWOD17MuUtbuPFfHtGQ1SIy6ykIYtBWaOCOP7uYy9efyad/+DR/9e0n6B3UqaUiMjspCGLSlMvw1fe+jo9ctpbvPLqH3//iLzRaqYjMSgqCGKVSxscuX8e3/uxihkvOu7/6EJ/8wVMc6xuqd2kiIqMUBDVw8dmL+NFHN/EfLl7Fvzz8Apf+7f18/ZfP0z+kwepEpP50h7Iae2ZfN39zz9P8audhFjVl+eCms7lu40raCg31Lk1E5rHJ7lCmIKiTzbuP8MWf7uAXOw6Ry6T4gwuWcd0bVnLhinbMrN7licg8oyCYxbbt7eKbv36R7z/2Mn1DJVYtKvD75y3lqvOW8uplrQoFEakKBcEc0DMwzD1P7OOeJ/fx0G8PUxpxlrfn2bR2MZvWdnDJKxfRXsjWu0wRmaMUBHPMkd4h7tu2n/u3H+ShnYfpGSxiBucuaeWiVe1cuGIBF61awOpFBbUYRGRKFARzWLE0wuN7jvGLHYfYsvsoW186xvHw4rT2QgPnLmnlnCUtnLu0hXOWtLLuzGYKWY0lKCJjTRYE+saY5TLpFK9btZDXrVoIQGnE2XnwOI++eJStLx7j2QM93LX5JfrD+yabwbK2PKsXF1i1qInVi8qPTaxcWCCfTdfznyMis5CCYI5Jp4xzlrRwzpIWrt24EoCREefFI308u7+H7ft72HXoOLsP93Hvk/s41jc85vXthQaWtDaytK2RJW358DGYPqOlkUXNWRYUsqRTOuQkkhQKgnkglTJWL25i9eImrnjN2PsgdPUN88KRXnYf7uOlI33s6+pnf9cA+7oGeGJPF4d7T77K2QwWFLIsasqyqDnLouYci5uyLGzKsbA5S3u+gbbwp70QPLY0Nig8ROYoBcE811Zo4PxCO+ef1T7u8oHhEge7B9nb1c+h44McPj7E4eODHO4dCp73DvLM3m4O9w7R1T887jYgCI+WXIa2QiQk8llaI6HRms/Q0thAa2Pw2JbP0NoYhEhjQ0od3yJ1oiBIuMaGNCsXFVi5qHDKdYeKIxztCwKhq3+Yrr7g8Vg43R0+HgvX2d/VTVd/ka7+IYZLk5+U0JC2MSHRms/Qkgsey2FRGSTlZa2NDTQ3ZtQiETlNCgKZsmwmxZmtjZzZ2jit17k7/cMlegaK9AwM09UfPHaH0939RboHhkefl5d19hwfne6dwrhMzbnM2CAZJzQmW5bLqFUiyaQgkNiZGYVshkI2M+0QKSuWRsIgCUKjuyI0xoZI8PxgzwA7D55YpzQyeaskm07R0pihNT9+y2RseDScFDotuQwptUpkDlIQyJyQSadY0JRlQdPpXV1dbpVUhkX3uEFSnh5mf/fA6LLyKboTMYPmbBAkLY3hYatxWh8t4eGs8UInl9HpvVJ7CgJJhGirZEnb6bVKhkdbJRWtj4Ei3f3jt0z2dQ2w/UBP0JLpH+YUjRKymVTQ55FLU8hmaIo8NmUzNOUyFLLpE4/leeHy8rKm8DHfkFYrRU5JQSAyRQ3pFAubsiycQaukb6hUcVhreDQkusuHvfqL9A0V6R0s0TdU5Fj/MC8f66dvsEjvUInewSLFUyVKRCE7NlROCplsmkIuQ3MkXAq5NIVsmnxDJnx9msaG9Oi2dJbX/KIgEKkRMwv+Ws9lWNo2s20NFUfoGypyfLBIXxgO0cdg/okw6R0qjQmSY/3D7D3WH7xmqEjfYImh0si0asiHwZDPpsc8L2SDlkg+O97yzNh1x6yXGZ1Wx31tKQhE5qBsJkU2k63qiLTlcOmNBErfUJGB4VL4vET/UIn+4fLzYJ3+4WB+eXlnzyB9Q8VgXrjuUHF6IZOyIGhyDWkaMykay88bUuTC6cZMeTp4LK8zurwhFa5TMS8ynYtMZ9PJDR8FgYgA0XCp/rZLIx4GSLEiTEqRMCmOBk45fAaGSwwMjzBYjD6WONI7NLpsYLjEYPHE4+kyY0y45BqCcAj2SxBA2UyabLr8/MTj2PXSJ56nUydtJ5tOkWtIj87LRbcTLs+ka3sXYQWBiMQunTKaw36IOI2MOEOlEQaHRxgolsYNi4HhEgPFEQajj2OWB49DpRGGiuFPuM2u/uFwXrB+eVl5ven03UwmZYwJlGwYKNdtXMkHN51dlfeIivVTMbMrgL8H0sDX3P0zFcstXH4V0Ad8wN0fjbMmEZm/UimjMRUcDmqj9vcBL434aCgMloJDYqOBEQmUoYpllaEyWCyNfU24zuLmXCx1xxYEZpYGvgxcDuwBNpvZ3e7+dGS1K4G14c8bgK+GjyIic046ZUHneDYNdQii0xXngaiNwE533+XuQ8CdwNUV61wNfMMDDwPtZrY0xppERKRCnEGwHHgpMr0nnDfddTCzG8xsi5lt6ezsrHqhIiJJFmcQjHceVmVPylTWwd1vdfcN7r6ho6OjKsWJiEggziDYA6yITJ8F7D2NdUREJEZxBsFmYK2ZrTGzLHANcHfFOncD77PAxUCXu++LsSYREakQ21lD7l40s5uA+whOH73N3beZ2Y3h8luAewlOHd1JcPro9XHVIyIi44v1OgJ3v5fgyz4675bIcwc+FGcNIiIyudpexywiIrOOBX+Uzx1m1gm8cJovXwwcqmI51TJb64LZW5vqmh7VNT3zsa5V7j7uaZdzLghmwsy2uPuGetdRabbWBbO3NtU1PaprepJWlw4NiYgknIJARCThkhYEt9a7gAnM1rpg9tamuqZHdU1PoupKVB+BiIicLGktAhERqaAgEBFJuMQEgZldYWbbzWynmd0c83utMLOfm9kzZrbNzP4inP8pM3vZzLaGP1dFXvPXYW3bzeytkfmvM7Mnw2VftCrcXdvMdofb3GpmW8J5C83s/5nZjvBxQS1rM7NzIvtlq5l1m9lH67HPzOw2MztoZk9F5lVt/5hZzszuCuf/2sxWz6Cuz5nZs2b2hJl9z8zaw/mrzaw/st9uibymFnVV7XOrcl13RWrabWZb67C/Jvp+qN/vmLvP+x+CsY5+C5wNZIHHgfUxvt9S4KLweQvwHLAe+BTwn8dZf31YUw5YE9aaDpf9BngjwZDdPwKurEJ9u4HFFfP+J3Bz+Pxm4LP1qC3yee0HVtVjnwFvBi4Cnopj/wD/CbglfH4NcNcM6noLkAmffzZS1+roehXbqUVdVfvcqllXxfLPA5+sw/6a6Puhbr9jSWkRTOVuaVXj7vs8vPeyu/cAzzDODXcirgbudPdBd3+eYBC+jRbcra3V3f/Ng0/0G8A7Yir7auCfwuf/FHmfetR2GfBbd5/sCvLY6nL3B4Ej47xftfZPdFvfBi6bSqtlvLrc/SfuXgwnHyYYyn1CtaprEnXdX2Xh6/8I+NZk24iprom+H+r2O5aUIJjSndDiEDbJLgR+Hc66KWzG3xZp+k1U3/LweeX8mXLgJ2b2iJndEM4708MhwMPHM+pUGwR/wUT/g86GfVbN/TP6mvBLvAtYVIUa/4Tgr8KyNWb2mJk9YGabIu9dq7qq9bnFsb82AQfcfUdkXs33V8X3Q91+x5ISBFO6E1rV39SsGfgO8FF37wa+CrwCeC2wj6BpOll9cdV9ibtfBFwJfMjM3jzJujWtzYJ7V7wd+D/hrNmyzyZyOnVUvUYz+wRQBL4ZztoHrHT3C4GPAXeYWWsN66rm5xbHZ3otY//YqPn+Guf7YcJVJ3ifqtWWlCCo+Z3QzKyB4EP+prt/F8DdD7h7yd1HgH8gOGQ1WX17GNvUr0rd7r43fDwIfC+s40DY1Cw3hw/WozaCcHrU3Q+ENc6KfUZ198/oa8wsA7Qx9UMrJzGz9wNvA94bHiIgPIxwOHz+CMFx5XW1qqvKn1u191cGeBdwV6Temu6v8b4fqOPvWFKCYCp3S6ua8Fjc14Fn3P3vIvOXRlZ7J1A+m+Fu4Jqwp38NsBb4Tdg87DGzi8Ntvg/4wQxrazKzlvJzgs7Gp8Ia3h+u9v7I+9SsttCYv9Rmwz6LvF+19k90W38I/Kz8BT5dZnYF8HHg7e7eF5nfYWbp8PnZYV27alhXNT+3qtUV+j3gWXcfPaxSy/010fcD9fwdm6wneT79ENwJ7TmCpP9EzO/1JoJm2BPA1vDnKuCfgSfD+XcDSyOv+URY23YiZ7kAGwj+E/0W+BLh1eAzqO1sgjMQHge2lfcFwfHDnwI7wseFdaitABwG2iLzar7PCIJoHzBM8JfVn1Zz/wCNBIe+dhKc9XH2DOraSXAsuPx7Vj5T5N3h5/s48CjwBzWuq2qfWzXrCuffDtxYsW4t99dE3w91+x3TEBMiIgmXlENDIiIyAQWBiEjCKQhERBJOQSAiknAKAhGRhFMQSGKZ2fHwcbWZXVflbf/XiumHqrl9kWpSEIgEI09OKwjKFx9NYkwQuPu/m2ZNIjWjIBCBzwCbLBiH/i/NLG3BOP+bw0HT/hzAzC61YBz5OwgulsLMvh8O3retPICfmX0GyIfb+2Y4r9z6sHDbT1kwjvx7Itu+38y+bcH9Bb4ZXi0qErtMvQsQmQVuJhg7/20A4Rd6l7u/3sxywK/M7CfhuhuB13gwHDDAn7j7ETPLA5vN7DvufrOZ3eTurx3nvd5FMBDbBcDi8DUPhssuBF5NMF7Mr4BLgF9W+x8rUkktApGTvQV4nwV3r/o1waX/a8Nlv4mEAMBHzOxxgnsBrIisN5E3Ad/yYEC2A8ADwOsj297jwUBtWwkOWYnETi0CkZMZ8GF3v2/MTLNLgd6K6d8D3ujufWZ2P8EYL6fa9kQGI89L6P+n1IhaBCLQQ3DLwLL7gP8YDhWMma0LR2qt1AYcDUPgVcDFkWXD5ddXeBB4T9gP0UFwO8XfVOVfIXKa9BeHSDAKZDE8xHM78PcEh2UeDTtsOxn/dpc/Bm40sycIRoV8OLLsVuAJM3vU3d8bmf89gnvMPk4wAuV/cff9YZCI1IVGHxURSTgdGhIRSTgFgYhIwikIREQSTkEgIpJwCgIRkYRTEIiIJJyCQEQk4f4/QDhJGyj+OJcAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plotting\n",
    "plt.plot(train_loss)\n",
    "\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83eb144a-8407-4132-9877-62a112fa750c",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "550a3dc6-ed5a-4f87-8e6e-895bb619a03b",
   "metadata": {},
   "source": [
    "# Pytorch neural network in practice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b0da2ae9-28d1-4bc2-ad46-29bb56f24a07",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x27dc9d70130>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "import torch\n",
    "from torch import nn \n",
    "import torch.nn.functional as F\n",
    "from torch.utils import data\n",
    "\n",
    "np.random.seed(0)\n",
    "torch.manual_seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58359a52-d63c-4d0e-893d-c25fd887821e",
   "metadata": {},
   "source": [
    "- ## 1. Read dataset and preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e13942c9-3ab0-4795-94c0-78752ad012bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fixed acidity</th>\n",
       "      <th>volatile acidity</th>\n",
       "      <th>citric acid</th>\n",
       "      <th>residual sugar</th>\n",
       "      <th>chlorides</th>\n",
       "      <th>free sulfur dioxide</th>\n",
       "      <th>total sulfur dioxide</th>\n",
       "      <th>density</th>\n",
       "      <th>pH</th>\n",
       "      <th>sulphates</th>\n",
       "      <th>alcohol</th>\n",
       "      <th>quality</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7.0</td>\n",
       "      <td>0.27</td>\n",
       "      <td>0.36</td>\n",
       "      <td>20.7</td>\n",
       "      <td>0.045</td>\n",
       "      <td>45.0</td>\n",
       "      <td>170.0</td>\n",
       "      <td>1.0010</td>\n",
       "      <td>3.00</td>\n",
       "      <td>0.45</td>\n",
       "      <td>8.8</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6.3</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.34</td>\n",
       "      <td>1.6</td>\n",
       "      <td>0.049</td>\n",
       "      <td>14.0</td>\n",
       "      <td>132.0</td>\n",
       "      <td>0.9940</td>\n",
       "      <td>3.30</td>\n",
       "      <td>0.49</td>\n",
       "      <td>9.5</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8.1</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.40</td>\n",
       "      <td>6.9</td>\n",
       "      <td>0.050</td>\n",
       "      <td>30.0</td>\n",
       "      <td>97.0</td>\n",
       "      <td>0.9951</td>\n",
       "      <td>3.26</td>\n",
       "      <td>0.44</td>\n",
       "      <td>10.1</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7.2</td>\n",
       "      <td>0.23</td>\n",
       "      <td>0.32</td>\n",
       "      <td>8.5</td>\n",
       "      <td>0.058</td>\n",
       "      <td>47.0</td>\n",
       "      <td>186.0</td>\n",
       "      <td>0.9956</td>\n",
       "      <td>3.19</td>\n",
       "      <td>0.40</td>\n",
       "      <td>9.9</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7.2</td>\n",
       "      <td>0.23</td>\n",
       "      <td>0.32</td>\n",
       "      <td>8.5</td>\n",
       "      <td>0.058</td>\n",
       "      <td>47.0</td>\n",
       "      <td>186.0</td>\n",
       "      <td>0.9956</td>\n",
       "      <td>3.19</td>\n",
       "      <td>0.40</td>\n",
       "      <td>9.9</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   fixed acidity  volatile acidity  citric acid  residual sugar  chlorides  \\\n",
       "0            7.0              0.27         0.36            20.7      0.045   \n",
       "1            6.3              0.30         0.34             1.6      0.049   \n",
       "2            8.1              0.28         0.40             6.9      0.050   \n",
       "3            7.2              0.23         0.32             8.5      0.058   \n",
       "4            7.2              0.23         0.32             8.5      0.058   \n",
       "\n",
       "   free sulfur dioxide  total sulfur dioxide  density    pH  sulphates  \\\n",
       "0                 45.0                 170.0   1.0010  3.00       0.45   \n",
       "1                 14.0                 132.0   0.9940  3.30       0.49   \n",
       "2                 30.0                  97.0   0.9951  3.26       0.44   \n",
       "3                 47.0                 186.0   0.9956  3.19       0.40   \n",
       "4                 47.0                 186.0   0.9956  3.19       0.40   \n",
       "\n",
       "   alcohol  quality  \n",
       "0      8.8        6  \n",
       "1      9.5        6  \n",
       "2     10.1        6  \n",
       "3      9.9        6  \n",
       "4      9.9        6  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wine_df = pd.read_csv('data/winequality-white.csv', delimiter=\";\")\n",
    "wine_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "03832150-2149-463e-899c-d8b5dec83d13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 4898 entries, 0 to 4897\n",
      "Data columns (total 12 columns):\n",
      " #   Column                Non-Null Count  Dtype  \n",
      "---  ------                --------------  -----  \n",
      " 0   fixed acidity         4898 non-null   float64\n",
      " 1   volatile acidity      4898 non-null   float64\n",
      " 2   citric acid           4898 non-null   float64\n",
      " 3   residual sugar        4898 non-null   float64\n",
      " 4   chlorides             4898 non-null   float64\n",
      " 5   free sulfur dioxide   4898 non-null   float64\n",
      " 6   total sulfur dioxide  4898 non-null   float64\n",
      " 7   density               4898 non-null   float64\n",
      " 8   pH                    4898 non-null   float64\n",
      " 9   sulphates             4898 non-null   float64\n",
      " 10  alcohol               4898 non-null   float64\n",
      " 11  quality               4898 non-null   int64  \n",
      "dtypes: float64(11), int64(1)\n",
      "memory usage: 459.3 KB\n"
     ]
    }
   ],
   "source": [
    "wine_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fcef0a58-b387-463b-99da-7ee84a16a00a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[6 5 7 8 4 3 9]\n",
      "\n",
      "[3 2 4 5 1 0 6]\n"
     ]
    }
   ],
   "source": [
    "# check how many classes we have\n",
    "# notice we don't have quality = 1 or 2. So we need to change the class idx to 0 - 6\n",
    "print(wine_df['quality'].unique())\n",
    "print()\n",
    "wine_df['quality'] = wine_df['quality'] - 3 # change the response value\n",
    "print(wine_df['quality'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9684ef10-e41c-4104-8590-3a929e2f0061",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of classes: 7\n",
      "number of features: 11\n"
     ]
    }
   ],
   "source": [
    "# we have 7 classes and 11 features\n",
    "n_class = len(wine_df['quality'].unique())\n",
    "n_features = wine_df.shape[1] - 1\n",
    "\n",
    "print('number of classes:', n_class)\n",
    "print('number of features:', n_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "257086a7-7eec-480b-bcff-46320a840c83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train-test split\n",
    "\n",
    "# extract features and responses\n",
    "X = wine_df.iloc[:, :-1].values\n",
    "y = wine_df['quality'].values\n",
    "\n",
    "# split into train and test set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "dca33694-e836-4096-9bb7-63e227f20248",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalization\n",
    "\n",
    "# NOTE: fit only on train set because you cannot use test set information for preprocessing!!\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "45216c49-319b-4094-9c52-2404c1e3729e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# further split into training and val set\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train,\n",
    "                                                  y_train,\n",
    "                                                  test_size=0.25,\n",
    "                                                  random_state=0) # 0.25 x 0.8 = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c37ba57a-2e3f-41aa-904a-26ece01deb00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert to torch object\n",
    "X_train = torch.from_numpy(X_train).float()\n",
    "y_train = torch.from_numpy(y_train).long()\n",
    "X_val = torch.from_numpy(X_val).float()\n",
    "y_val = torch.from_numpy(y_val).long()\n",
    "X_test = torch.from_numpy(X_test).float()\n",
    "y_test = torch.from_numpy(y_test).long()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "caf4cf7a-25bc-4fe2-b894-a7dee7572cc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train size: torch.Size([2938, 11])\n",
      "val size: torch.Size([980, 11])\n",
      "test size: torch.Size([980, 11])\n"
     ]
    }
   ],
   "source": [
    "# print the size for train, val and test data\n",
    "print('train size:', X_train.shape)\n",
    "print('val size:', X_val.shape)\n",
    "print('test size:', X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "830efcfd-937b-4998-8e56-90001903a54a",
   "metadata": {},
   "source": [
    "- ## 2. Define a neural network model class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ec1a25de-e0b3-4329-af27-7487d57d6735",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NNClassifier(torch.nn.Module):\n",
    "    def __init__(self, inputSize, hiddenSize, outputSize):\n",
    "        '''\n",
    "            inputSize: input dim\n",
    "            hiddenSize: hidden dim\n",
    "            outputSize: output dim (should be number of classes)\n",
    "        '''\n",
    "        super().__init__()\n",
    "\n",
    "        # here we only set 1 hidden layer, you can add more if you want\n",
    "        self.Linear1 = nn.Linear(inputSize, hiddenSize) \n",
    "        self.Linear2 = nn.Linear(hiddenSize, outputSize)\n",
    "\n",
    "        # relu activation\n",
    "        self.act1 = nn.ReLU()\n",
    "        \n",
    "                    \n",
    "    def forward(self, x):\n",
    "        \n",
    "        # apply the first linear layer\n",
    "        x = self.Linear1(x)\n",
    "        \n",
    "        # apply the activation\n",
    "        x = self.act1(x)\n",
    "\n",
    "        # apply the second linear layer\n",
    "        # (we don't need to apply activation for the output layer)\n",
    "        x = self.Linear2(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "88b74954-6428-4627-98c9-a24fe88a9f1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize the model\n",
    "model = NNClassifier(n_features, 8, n_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "fab2826f-d101-4dbb-807d-b0f5f908695d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear1.weight torch.Size([8, 11])\n",
      "Linear1.bias torch.Size([8])\n",
      "Linear2.weight torch.Size([7, 8])\n",
      "Linear2.bias torch.Size([7])\n"
     ]
    }
   ],
   "source": [
    "# let's see how many parameters we have\n",
    "for name, param in model.named_parameters():\n",
    "    print(name, param.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc394a34-411d-4076-8575-d5c65778be6f",
   "metadata": {},
   "source": [
    "- ## 3. Define loss and optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b85c38ea-0160-41fe-8b7c-3221e201a697",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f974da31-088a-45bf-90dc-968952a89210",
   "metadata": {},
   "source": [
    "- ## 4. Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2338f620-e755-460a-87ce-7b31f6f86659",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 2.0727 (Train) 0.2388 (Val acc)\n",
      "Epoch 50: 1.2229 (Train) 0.4531 (Val acc)\n",
      "Epoch 100: 1.2112 (Train) 0.4531 (Val acc)\n",
      "Epoch 150: 1.1794 (Train) 0.4990 (Val acc)\n",
      "Epoch 200: 1.1413 (Train) 0.5224 (Val acc)\n",
      "Epoch 250: 1.1745 (Train) 0.5347 (Val acc)\n",
      "Epoch 300: 1.2209 (Train) 0.5296 (Val acc)\n",
      "Epoch 350: 1.0527 (Train) 0.5408 (Val acc)\n",
      "Epoch 400: 1.0713 (Train) 0.5378 (Val acc)\n",
      "Epoch 450: 1.1029 (Train) 0.5490 (Val acc)\n",
      "Epoch 500: 1.0814 (Train) 0.5480 (Val acc)\n",
      "Epoch 550: 1.1757 (Train) 0.5490 (Val acc)\n",
      "Epoch 600: 1.0313 (Train) 0.5408 (Val acc)\n",
      "Epoch 650: 1.0525 (Train) 0.5327 (Val acc)\n",
      "Epoch 700: 1.0044 (Train) 0.5408 (Val acc)\n",
      "Epoch 750: 1.1190 (Train) 0.5265 (Val acc)\n",
      "Epoch 800: 1.0406 (Train) 0.5378 (Val acc)\n",
      "Epoch 850: 1.0775 (Train) 0.5378 (Val acc)\n",
      "Epoch 900: 1.1330 (Train) 0.5286 (Val acc)\n",
      "Epoch 950: 1.1611 (Train) 0.5480 (Val acc)\n",
      "Epoch 1000: 1.0982 (Train) 0.5551 (Val acc)\n",
      "Epoch 1050: 1.0449 (Train) 0.5500 (Val acc)\n",
      "Epoch 1100: 1.1656 (Train) 0.5571 (Val acc)\n",
      "Epoch 1150: 1.0096 (Train) 0.5439 (Val acc)\n",
      "Epoch 1200: 1.0276 (Train) 0.5429 (Val acc)\n",
      "Epoch 1250: 0.9713 (Train) 0.5561 (Val acc)\n",
      "Epoch 1300: 1.2230 (Train) 0.5429 (Val acc)\n",
      "Epoch 1350: 1.0939 (Train) 0.5541 (Val acc)\n",
      "Epoch 1400: 1.0351 (Train) 0.5449 (Val acc)\n",
      "Epoch 1450: 1.0630 (Train) 0.5378 (Val acc)\n",
      "Epoch 1500: 1.0853 (Train) 0.5500 (Val acc)\n",
      "Epoch 1550: 1.0755 (Train) 0.5531 (Val acc)\n",
      "Epoch 1600: 1.0721 (Train) 0.5510 (Val acc)\n",
      "Epoch 1650: 1.0782 (Train) 0.5602 (Val acc)\n",
      "Epoch 1700: 0.9954 (Train) 0.5500 (Val acc)\n",
      "Epoch 1750: 1.1277 (Train) 0.5541 (Val acc)\n",
      "Epoch 1800: 1.0936 (Train) 0.5418 (Val acc)\n",
      "Epoch 1850: 1.1095 (Train) 0.5520 (Val acc)\n",
      "Epoch 1900: 1.0447 (Train) 0.5490 (Val acc)\n",
      "Epoch 1950: 1.0284 (Train) 0.5520 (Val acc)\n",
      "Epoch 2000: 1.0351 (Train) 0.5551 (Val acc)\n",
      "Epoch 2050: 1.1609 (Train) 0.5510 (Val acc)\n",
      "Epoch 2100: 1.1117 (Train) 0.5510 (Val acc)\n",
      "Epoch 2150: 1.1141 (Train) 0.5510 (Val acc)\n",
      "Epoch 2200: 1.1025 (Train) 0.5500 (Val acc)\n",
      "Epoch 2250: 1.0978 (Train) 0.5531 (Val acc)\n",
      "Epoch 2300: 1.0955 (Train) 0.5510 (Val acc)\n",
      "Epoch 2350: 1.1022 (Train) 0.5541 (Val acc)\n",
      "Epoch 2400: 1.0717 (Train) 0.5520 (Val acc)\n",
      "Epoch 2450: 0.9801 (Train) 0.5286 (Val acc)\n",
      "Epoch 2500: 1.0858 (Train) 0.5520 (Val acc)\n",
      "Epoch 2550: 1.1228 (Train) 0.5561 (Val acc)\n",
      "Epoch 2600: 0.9989 (Train) 0.5602 (Val acc)\n",
      "Epoch 2650: 1.0890 (Train) 0.5551 (Val acc)\n",
      "Epoch 2700: 1.0488 (Train) 0.5480 (Val acc)\n",
      "Epoch 2750: 1.0121 (Train) 0.5510 (Val acc)\n",
      "Epoch 2800: 1.0073 (Train) 0.5561 (Val acc)\n",
      "Epoch 2850: 1.1204 (Train) 0.5551 (Val acc)\n",
      "Epoch 2900: 1.0730 (Train) 0.5541 (Val acc)\n",
      "Epoch 2950: 1.1446 (Train) 0.5520 (Val acc)\n",
      "Epoch 3000: 1.1374 (Train) 0.5469 (Val acc)\n",
      "Epoch 3050: 1.0503 (Train) 0.5459 (Val acc)\n",
      "Epoch 3100: 1.0600 (Train) 0.5510 (Val acc)\n",
      "Epoch 3150: 1.0391 (Train) 0.5510 (Val acc)\n",
      "Epoch 3200: 1.0298 (Train) 0.5490 (Val acc)\n",
      "Epoch 3250: 1.1968 (Train) 0.5429 (Val acc)\n",
      "Epoch 3300: 1.0540 (Train) 0.5429 (Val acc)\n",
      "Epoch 3350: 1.1337 (Train) 0.5480 (Val acc)\n",
      "Epoch 3400: 1.1693 (Train) 0.5439 (Val acc)\n",
      "Epoch 3450: 1.0863 (Train) 0.5316 (Val acc)\n",
      "Epoch 3500: 1.1038 (Train) 0.5500 (Val acc)\n",
      "Epoch 3550: 1.0811 (Train) 0.5459 (Val acc)\n",
      "Epoch 3600: 1.0840 (Train) 0.5469 (Val acc)\n",
      "Epoch 3650: 1.0840 (Train) 0.5459 (Val acc)\n",
      "Epoch 3700: 1.0513 (Train) 0.5439 (Val acc)\n",
      "Epoch 3750: 1.1009 (Train) 0.5500 (Val acc)\n",
      "Epoch 3800: 1.0528 (Train) 0.5439 (Val acc)\n",
      "Epoch 3850: 1.0411 (Train) 0.5480 (Val acc)\n",
      "Epoch 3900: 1.0837 (Train) 0.5490 (Val acc)\n",
      "Epoch 3950: 1.0700 (Train) 0.5449 (Val acc)\n",
      "Epoch 4000: 1.0761 (Train) 0.5480 (Val acc)\n",
      "Epoch 4050: 1.0398 (Train) 0.5490 (Val acc)\n",
      "Epoch 4100: 1.0674 (Train) 0.5520 (Val acc)\n",
      "Epoch 4150: 1.0657 (Train) 0.5398 (Val acc)\n",
      "Epoch 4200: 1.0714 (Train) 0.5480 (Val acc)\n",
      "Epoch 4250: 1.1533 (Train) 0.5480 (Val acc)\n",
      "Epoch 4300: 1.0735 (Train) 0.5459 (Val acc)\n",
      "Epoch 4350: 1.1035 (Train) 0.5439 (Val acc)\n",
      "Epoch 4400: 1.0210 (Train) 0.5418 (Val acc)\n",
      "Epoch 4450: 1.1505 (Train) 0.5449 (Val acc)\n",
      "Epoch 4500: 1.0736 (Train) 0.5429 (Val acc)\n",
      "Epoch 4550: 1.0763 (Train) 0.5459 (Val acc)\n",
      "Epoch 4600: 1.0561 (Train) 0.5429 (Val acc)\n",
      "Epoch 4650: 1.1167 (Train) 0.5469 (Val acc)\n",
      "Epoch 4700: 1.0787 (Train) 0.5449 (Val acc)\n",
      "Epoch 4750: 1.1068 (Train) 0.5459 (Val acc)\n",
      "Epoch 4800: 1.0743 (Train) 0.5500 (Val acc)\n",
      "Epoch 4850: 1.1358 (Train) 0.5469 (Val acc)\n",
      "Epoch 4900: 1.0770 (Train) 0.5449 (Val acc)\n",
      "Epoch 4950: 1.0145 (Train) 0.5490 (Val acc)\n",
      "Epoch 5000: 1.0345 (Train) 0.5439 (Val acc)\n",
      "Epoch 5050: 1.1017 (Train) 0.5469 (Val acc)\n",
      "Epoch 5100: 1.0912 (Train) 0.5449 (Val acc)\n",
      "Epoch 5150: 1.0616 (Train) 0.5429 (Val acc)\n",
      "Epoch 5200: 1.1115 (Train) 0.5480 (Val acc)\n",
      "Epoch 5250: 0.9518 (Train) 0.5500 (Val acc)\n",
      "Epoch 5300: 1.1873 (Train) 0.5429 (Val acc)\n",
      "Epoch 5350: 1.0583 (Train) 0.5480 (Val acc)\n",
      "Epoch 5400: 1.1765 (Train) 0.5449 (Val acc)\n",
      "Epoch 5450: 1.0588 (Train) 0.5429 (Val acc)\n",
      "Epoch 5500: 1.0560 (Train) 0.5449 (Val acc)\n",
      "Epoch 5550: 1.0541 (Train) 0.5490 (Val acc)\n",
      "Epoch 5600: 1.1592 (Train) 0.5459 (Val acc)\n",
      "Epoch 5650: 1.0551 (Train) 0.5408 (Val acc)\n",
      "Epoch 5700: 1.0227 (Train) 0.5531 (Val acc)\n",
      "Epoch 5750: 1.0183 (Train) 0.5439 (Val acc)\n",
      "Epoch 5800: 1.0563 (Train) 0.5500 (Val acc)\n",
      "Epoch 5850: 1.1604 (Train) 0.5480 (Val acc)\n",
      "Epoch 5900: 0.9519 (Train) 0.5449 (Val acc)\n",
      "Epoch 5950: 1.1037 (Train) 0.5439 (Val acc)\n",
      "Epoch 6000: 0.9842 (Train) 0.5408 (Val acc)\n",
      "Epoch 6050: 1.1050 (Train) 0.5449 (Val acc)\n",
      "Epoch 6100: 1.0703 (Train) 0.5510 (Val acc)\n",
      "Epoch 6150: 1.0125 (Train) 0.5500 (Val acc)\n",
      "Epoch 6200: 1.0006 (Train) 0.5520 (Val acc)\n",
      "Epoch 6250: 1.0817 (Train) 0.5480 (Val acc)\n",
      "Epoch 6300: 1.1130 (Train) 0.5439 (Val acc)\n",
      "Epoch 6350: 1.1123 (Train) 0.5510 (Val acc)\n",
      "Epoch 6400: 1.0425 (Train) 0.5490 (Val acc)\n",
      "Epoch 6450: 1.1337 (Train) 0.5469 (Val acc)\n",
      "Epoch 6500: 0.9735 (Train) 0.5490 (Val acc)\n",
      "Epoch 6550: 1.0955 (Train) 0.5490 (Val acc)\n",
      "Epoch 6600: 1.1379 (Train) 0.5469 (Val acc)\n",
      "Epoch 6650: 0.9795 (Train) 0.5449 (Val acc)\n",
      "Epoch 6700: 1.0678 (Train) 0.5418 (Val acc)\n",
      "Epoch 6750: 1.1127 (Train) 0.5561 (Val acc)\n",
      "Epoch 6800: 1.1094 (Train) 0.5520 (Val acc)\n",
      "Epoch 6850: 1.0360 (Train) 0.5520 (Val acc)\n",
      "Epoch 6900: 1.0438 (Train) 0.5500 (Val acc)\n",
      "Epoch 6950: 1.0649 (Train) 0.5500 (Val acc)\n",
      "Epoch 7000: 1.0536 (Train) 0.5469 (Val acc)\n",
      "Epoch 7050: 1.0726 (Train) 0.5582 (Val acc)\n",
      "Epoch 7100: 1.1021 (Train) 0.5520 (Val acc)\n",
      "Epoch 7150: 1.1185 (Train) 0.5520 (Val acc)\n",
      "Epoch 7200: 1.1110 (Train) 0.5551 (Val acc)\n",
      "Epoch 7250: 1.0349 (Train) 0.5490 (Val acc)\n",
      "Epoch 7300: 1.0352 (Train) 0.5469 (Val acc)\n",
      "Epoch 7350: 1.0722 (Train) 0.5469 (Val acc)\n",
      "Epoch 7400: 1.0093 (Train) 0.5490 (Val acc)\n",
      "Epoch 7450: 0.9861 (Train) 0.5531 (Val acc)\n",
      "Epoch 7500: 1.0152 (Train) 0.5602 (Val acc)\n",
      "Epoch 7550: 1.0555 (Train) 0.5551 (Val acc)\n",
      "Epoch 7600: 1.0566 (Train) 0.5531 (Val acc)\n",
      "Epoch 7650: 0.9889 (Train) 0.5531 (Val acc)\n",
      "Epoch 7700: 1.0346 (Train) 0.5520 (Val acc)\n",
      "Epoch 7750: 1.0111 (Train) 0.5541 (Val acc)\n",
      "Epoch 7800: 1.0173 (Train) 0.5541 (Val acc)\n",
      "Epoch 7850: 1.0795 (Train) 0.5520 (Val acc)\n",
      "Epoch 7900: 1.1068 (Train) 0.5531 (Val acc)\n",
      "Epoch 7950: 1.1181 (Train) 0.5500 (Val acc)\n",
      "Epoch 8000: 1.0154 (Train) 0.5520 (Val acc)\n",
      "Epoch 8050: 1.0769 (Train) 0.5551 (Val acc)\n",
      "Epoch 8100: 1.0321 (Train) 0.5520 (Val acc)\n",
      "Epoch 8150: 1.1299 (Train) 0.5561 (Val acc)\n",
      "Epoch 8200: 1.0618 (Train) 0.5367 (Val acc)\n",
      "Epoch 8250: 1.1188 (Train) 0.5612 (Val acc)\n",
      "Epoch 8300: 1.0000 (Train) 0.5582 (Val acc)\n",
      "Epoch 8350: 1.0367 (Train) 0.5469 (Val acc)\n",
      "Epoch 8400: 1.0403 (Train) 0.5520 (Val acc)\n",
      "Epoch 8450: 1.0557 (Train) 0.5561 (Val acc)\n",
      "Epoch 8500: 1.0788 (Train) 0.5571 (Val acc)\n",
      "Epoch 8550: 0.9606 (Train) 0.5531 (Val acc)\n",
      "Epoch 8600: 1.0496 (Train) 0.5520 (Val acc)\n",
      "Epoch 8650: 0.9941 (Train) 0.5418 (Val acc)\n",
      "Epoch 8700: 1.0542 (Train) 0.5439 (Val acc)\n",
      "Epoch 8750: 1.1289 (Train) 0.5439 (Val acc)\n",
      "Epoch 8800: 1.0455 (Train) 0.5541 (Val acc)\n",
      "Epoch 8850: 1.0745 (Train) 0.5561 (Val acc)\n",
      "Epoch 8900: 1.0206 (Train) 0.5551 (Val acc)\n",
      "Epoch 8950: 0.9891 (Train) 0.5490 (Val acc)\n",
      "Epoch 9000: 1.0142 (Train) 0.5602 (Val acc)\n",
      "Epoch 9050: 1.1668 (Train) 0.5510 (Val acc)\n",
      "Epoch 9100: 1.0035 (Train) 0.5439 (Val acc)\n",
      "Epoch 9150: 1.0545 (Train) 0.5449 (Val acc)\n",
      "Epoch 9200: 1.0243 (Train) 0.5612 (Val acc)\n",
      "Epoch 9250: 1.1038 (Train) 0.5357 (Val acc)\n",
      "Epoch 9300: 1.0273 (Train) 0.5592 (Val acc)\n",
      "Epoch 9350: 1.1143 (Train) 0.5633 (Val acc)\n",
      "Epoch 9400: 1.1036 (Train) 0.5582 (Val acc)\n",
      "Epoch 9450: 0.9981 (Train) 0.5551 (Val acc)\n",
      "Epoch 9500: 1.0307 (Train) 0.5469 (Val acc)\n",
      "Epoch 9550: 1.0845 (Train) 0.5571 (Val acc)\n",
      "Epoch 9600: 1.0545 (Train) 0.5347 (Val acc)\n",
      "Epoch 9650: 1.1718 (Train) 0.5480 (Val acc)\n",
      "Epoch 9700: 1.0044 (Train) 0.5398 (Val acc)\n",
      "Epoch 9750: 0.9733 (Train) 0.5531 (Val acc)\n",
      "Epoch 9800: 1.0909 (Train) 0.5429 (Val acc)\n",
      "Epoch 9850: 1.0356 (Train) 0.5561 (Val acc)\n",
      "Epoch 9900: 1.0816 (Train) 0.5541 (Val acc)\n",
      "Epoch 9950: 1.0108 (Train) 0.5520 (Val acc)\n"
     ]
    }
   ],
   "source": [
    "# select batch size, usually as a power of 2\n",
    "N = X_train.shape[0]\n",
    "batchSize = 256\n",
    "\n",
    "# ====== Step 3 =========\n",
    "epochs = 10000\n",
    "for i in range(epochs):\n",
    "    \n",
    "    # select a random batch\n",
    "    randomIdx = torch.randint(0, N, (batchSize,))\n",
    "    x_batch = X_train[randomIdx]\n",
    "    y_batch = y_train[randomIdx]\n",
    "    \n",
    "    # zero the parameter gradients\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # calulate output and loss \n",
    "    output = model(x_batch)\n",
    "    loss = loss_fn(output, y_batch)\n",
    "\n",
    "    # backprop and take a step\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if i % 50 == 0:\n",
    "        \n",
    "        # Note we need to deactivate training (by not require gradient) and move to validation phase\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            output_val = model(X_val)\n",
    "\n",
    "            # calculate prediction and accuracy\n",
    "            y_pred = torch.argmax(output_val, dim = 1) # find out the class prediction\n",
    "            acc = (y_pred == y_val).float().sum()/y_val.shape[0]\n",
    "        \n",
    "        model.train() \n",
    "\n",
    "        print('Epoch {}: {:.4f} (Train) {:.4f} (Val acc)'.format(i, loss, acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "184bdfd1-9742-4f70-be26-8d1f94674c69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00         9\n",
      "           1       1.00      0.06      0.11        51\n",
      "           2       0.59      0.46      0.52       295\n",
      "           3       0.48      0.76      0.59       409\n",
      "           4       0.49      0.26      0.34       183\n",
      "           5       0.00      0.00      0.00        33\n",
      "\n",
      "    accuracy                           0.51       980\n",
      "   macro avg       0.43      0.26      0.26       980\n",
      "weighted avg       0.52      0.51      0.47       980\n",
      "\n",
      "[[  0   0   3   6   0   0]\n",
      " [  0   3  25  21   2   0]\n",
      " [  0   0 136 157   2   0]\n",
      " [  0   0  63 311  35   0]\n",
      " [  0   0   4 132  47   0]\n",
      " [  0   0   0  24   9   0]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Public\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\Public\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\Public\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "# predict test data for NN\n",
    "output_test = model(X_test)\n",
    "y_pred = torch.argmax(output_test, dim = 1)\n",
    "\n",
    "print(classification_report(y_test,y_pred))\n",
    "print(confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "6c385ffb-471e-490a-86ee-e8d45a2c6a1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00         9\n",
      "           1       0.00      0.00      0.00        51\n",
      "           2       0.54      0.44      0.48       295\n",
      "           3       0.47      0.82      0.60       409\n",
      "           4       0.62      0.13      0.21       183\n",
      "           5       0.00      0.00      0.00        33\n",
      "\n",
      "    accuracy                           0.50       980\n",
      "   macro avg       0.27      0.23      0.22       980\n",
      "weighted avg       0.48      0.50      0.44       980\n",
      "\n",
      "[[  0   0   5   4   0   0]\n",
      " [  0   0  27  23   1   0]\n",
      " [  0   0 129 166   0   0]\n",
      " [  0   0  66 334   9   0]\n",
      " [  0   0  11 149  23   0]\n",
      " [  0   0   0  29   4   0]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Public\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\Public\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\Public\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\Public\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "# let's compare against a baseline model logistic regression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "lg_clf = LogisticRegression().fit(X_train, y_train)\n",
    "y_pred = lg_clf.predict(X_test)\n",
    "\n",
    "print(classification_report(y_test,y_pred))\n",
    "print(confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "8270793c-5f4f-45c6-a19a-190b5b6222e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Currently, the prediction from our neural network is not good. This may be because the hyperparameters are not tuned."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

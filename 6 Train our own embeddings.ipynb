{
    "metadata": {
        "kernelspec": {
            "name": "python3",
            "display_name": "Python 3 (ipykernel)",
            "language": "python"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4,
    "cells": [
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Embedding layer for word embedding"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 66,
            "metadata": {},
            "outputs": [],
            "source": [
                "import pandas as pd\n",
                "import numpy as np\n",
                "from sklearn.preprocessing import MinMaxScaler\n",
                "from sklearn.model_selection import train_test_split\n",
                "from sklearn.metrics import classification_report\n",
                "from sklearn.metrics import confusion_matrix\n",
                "\n",
                "import torch\n",
                "from torch import nn \n",
                "import torch.nn.functional as F\n",
                "from torch import utils\n",
                "\n",
                "torch.manual_seed(0)\n",
                "np.random.seed(0)"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "**Read data**"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 67,
            "metadata": {
                "scrolled": true
            },
            "outputs": [
                {
                    "data": {
                        "text/html": [
                            "\u003cdiv\u003e\n",
                            "\u003cstyle scoped\u003e\n",
                            "    .dataframe tbody tr th:only-of-type {\n",
                            "        vertical-align: middle;\n",
                            "    }\n",
                            "\n",
                            "    .dataframe tbody tr th {\n",
                            "        vertical-align: top;\n",
                            "    }\n",
                            "\n",
                            "    .dataframe thead th {\n",
                            "        text-align: right;\n",
                            "    }\n",
                            "\u003c/style\u003e\n",
                            "\u003ctable border=\"1\" class=\"dataframe\"\u003e\n",
                            "  \u003cthead\u003e\n",
                            "    \u003ctr style=\"text-align: right;\"\u003e\n",
                            "      \u003cth\u003e\u003c/th\u003e\n",
                            "      \u003cth\u003eCategory\u003c/th\u003e\n",
                            "      \u003cth\u003eMessage\u003c/th\u003e\n",
                            "    \u003c/tr\u003e\n",
                            "  \u003c/thead\u003e\n",
                            "  \u003ctbody\u003e\n",
                            "    \u003ctr\u003e\n",
                            "      \u003cth\u003e0\u003c/th\u003e\n",
                            "      \u003ctd\u003eham\u003c/td\u003e\n",
                            "      \u003ctd\u003eGo until jurong point, crazy.. Available only ...\u003c/td\u003e\n",
                            "    \u003c/tr\u003e\n",
                            "    \u003ctr\u003e\n",
                            "      \u003cth\u003e1\u003c/th\u003e\n",
                            "      \u003ctd\u003eham\u003c/td\u003e\n",
                            "      \u003ctd\u003eOk lar... Joking wif u oni...\u003c/td\u003e\n",
                            "    \u003c/tr\u003e\n",
                            "    \u003ctr\u003e\n",
                            "      \u003cth\u003e2\u003c/th\u003e\n",
                            "      \u003ctd\u003espam\u003c/td\u003e\n",
                            "      \u003ctd\u003eFree entry in 2 a wkly comp to win FA Cup fina...\u003c/td\u003e\n",
                            "    \u003c/tr\u003e\n",
                            "    \u003ctr\u003e\n",
                            "      \u003cth\u003e3\u003c/th\u003e\n",
                            "      \u003ctd\u003eham\u003c/td\u003e\n",
                            "      \u003ctd\u003eU dun say so early hor... U c already then say...\u003c/td\u003e\n",
                            "    \u003c/tr\u003e\n",
                            "    \u003ctr\u003e\n",
                            "      \u003cth\u003e4\u003c/th\u003e\n",
                            "      \u003ctd\u003eham\u003c/td\u003e\n",
                            "      \u003ctd\u003eNah I don't think he goes to usf, he lives aro...\u003c/td\u003e\n",
                            "    \u003c/tr\u003e\n",
                            "  \u003c/tbody\u003e\n",
                            "\u003c/table\u003e\n",
                            "\u003c/div\u003e"
                        ],
                        "text/plain": [
                            "  Category                                            Message\n",
                            "0      ham  Go until jurong point, crazy.. Available only ...\n",
                            "1      ham                      Ok lar... Joking wif u oni...\n",
                            "2     spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
                            "3      ham  U dun say so early hor... U c already then say...\n",
                            "4      ham  Nah I don't think he goes to usf, he lives aro..."
                        ]
                    },
                    "execution_count": 67,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "# first let us load a dataset called spam.csv\n",
                "# Downloaded from https://www.kaggle.com/team-ai/spam-text-message-classification?select=SPAM+text+message+20170820+-+Data.csv\n",
                "\n",
                "data = pd.read_csv(\"spam.csv\")\n",
                "data.head()"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "This dataset collects text messages and the task is to classify whether a message is a spam or not."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 68,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "\u003cclass 'pandas.core.frame.DataFrame'\u003e\nRangeIndex: 5572 entries, 0 to 5571\nData columns (total 2 columns):\n #   Column    Non-Null Count  Dtype \n---  ------    --------------  ----- \n 0   Category  5572 non-null   object\n 1   Message   5572 non-null   object\ndtypes: object(2)\nmemory usage: 87.2+ KB\n"
                }
            ],
            "source": [
                "# some basic information\n",
                "data.info()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 69,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "ham     4825\n",
                            "spam     747\n",
                            "Name: Category, dtype: int64"
                        ]
                    },
                    "execution_count": 69,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "# This dataset is imbalanced\n",
                "data['Category'].value_counts()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 70,
            "metadata": {
                "scrolled": true
            },
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "['ham' 'spam']\n"
                },
                {
                    "data": {
                        "text/html": [
                            "\u003cdiv\u003e\n",
                            "\u003cstyle scoped\u003e\n",
                            "    .dataframe tbody tr th:only-of-type {\n",
                            "        vertical-align: middle;\n",
                            "    }\n",
                            "\n",
                            "    .dataframe tbody tr th {\n",
                            "        vertical-align: top;\n",
                            "    }\n",
                            "\n",
                            "    .dataframe thead th {\n",
                            "        text-align: right;\n",
                            "    }\n",
                            "\u003c/style\u003e\n",
                            "\u003ctable border=\"1\" class=\"dataframe\"\u003e\n",
                            "  \u003cthead\u003e\n",
                            "    \u003ctr style=\"text-align: right;\"\u003e\n",
                            "      \u003cth\u003e\u003c/th\u003e\n",
                            "      \u003cth\u003eCategory\u003c/th\u003e\n",
                            "      \u003cth\u003eMessage\u003c/th\u003e\n",
                            "    \u003c/tr\u003e\n",
                            "  \u003c/thead\u003e\n",
                            "  \u003ctbody\u003e\n",
                            "    \u003ctr\u003e\n",
                            "      \u003cth\u003e0\u003c/th\u003e\n",
                            "      \u003ctd\u003e0\u003c/td\u003e\n",
                            "      \u003ctd\u003eGo until jurong point, crazy.. Available only ...\u003c/td\u003e\n",
                            "    \u003c/tr\u003e\n",
                            "    \u003ctr\u003e\n",
                            "      \u003cth\u003e1\u003c/th\u003e\n",
                            "      \u003ctd\u003e0\u003c/td\u003e\n",
                            "      \u003ctd\u003eOk lar... Joking wif u oni...\u003c/td\u003e\n",
                            "    \u003c/tr\u003e\n",
                            "    \u003ctr\u003e\n",
                            "      \u003cth\u003e2\u003c/th\u003e\n",
                            "      \u003ctd\u003e1\u003c/td\u003e\n",
                            "      \u003ctd\u003eFree entry in 2 a wkly comp to win FA Cup fina...\u003c/td\u003e\n",
                            "    \u003c/tr\u003e\n",
                            "    \u003ctr\u003e\n",
                            "      \u003cth\u003e3\u003c/th\u003e\n",
                            "      \u003ctd\u003e0\u003c/td\u003e\n",
                            "      \u003ctd\u003eU dun say so early hor... U c already then say...\u003c/td\u003e\n",
                            "    \u003c/tr\u003e\n",
                            "    \u003ctr\u003e\n",
                            "      \u003cth\u003e4\u003c/th\u003e\n",
                            "      \u003ctd\u003e0\u003c/td\u003e\n",
                            "      \u003ctd\u003eNah I don't think he goes to usf, he lives aro...\u003c/td\u003e\n",
                            "    \u003c/tr\u003e\n",
                            "  \u003c/tbody\u003e\n",
                            "\u003c/table\u003e\n",
                            "\u003c/div\u003e"
                        ],
                        "text/plain": [
                            "   Category                                            Message\n",
                            "0         0  Go until jurong point, crazy.. Available only ...\n",
                            "1         0                      Ok lar... Joking wif u oni...\n",
                            "2         1  Free entry in 2 a wkly comp to win FA Cup fina...\n",
                            "3         0  U dun say so early hor... U c already then say...\n",
                            "4         0  Nah I don't think he goes to usf, he lives aro..."
                        ]
                    },
                    "execution_count": 70,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "# preprocessing data\n",
                "from sklearn.preprocessing import LabelEncoder\n",
                "\n",
                "le = LabelEncoder() \n",
                "target = le.fit_transform(data['Category']) # convert target into integers\n",
                "data['Category'] = target\n",
                "print(le.classes_) # this shows which index maps to which class\n",
                "\n",
                "data.head()"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "**Preprocessing**\n",
                "\n",
                "Recall in Task 1, we construct a Torch dataset by passing in X and y. Here we cannot do that because features are not numerical. The feature now is text and we cannot convert them to a torch Tensor. Thus, we may need to define our own dataset class, which should inherit from the parent class `utils.data.Dataset`. \n",
                "\n",
                "Before that, we should construct training, validation and test sets. Here we don't split them into X and y."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 71,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "(1114, 2)\n(1114, 2)\n(3344, 2)\n"
                }
            ],
            "source": [
                "np.random.seed(0)\n",
                "\n",
                "index = list(range(data.shape[0])) # an list of indices\n",
                "np.random.shuffle(index) # shuffle the index in-place\n",
                "\n",
                "p_val = 0.2\n",
                "p_test = 0.2\n",
                "N_test = int(data.shape[0] * p_test)\n",
                "N_val = int(data.shape[0] * p_val)\n",
                "\n",
                "# get training, val and test sets\n",
                "test_data = data.iloc[ index[:N_test] ,:]\n",
                "val_data = data.iloc[ index[N_test: (N_test+N_val)], :]\n",
                "train_data = data.iloc[ index[(N_test+N_val):], :]\n",
                "\n",
                "print(test_data.shape)\n",
                "print(val_data.shape)\n",
                "print(train_data.shape)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 72,
            "metadata": {},
            "outputs": [],
            "source": [
                "# define our own torch dataset\n",
                "# for a torch dataset, we need to define two functions: \n",
                "#     __len__: return the length of dataset\n",
                "#     __getitem__: given a index (integer), return the corresponding sample, both y and X\n",
                "\n",
                "class SpamDataset(utils.data.Dataset):\n",
                "    def __init__(self, myData):\n",
                "        \"\"\"\n",
                "        myData should be a dataframe object containing both y (first col) and X (second col)\n",
                "        \"\"\"\n",
                "        super().__init__()\n",
                "        self.data = myData\n",
                "        \n",
                "    def __len__(self):\n",
                "        return len(self.data)\n",
                "    \n",
                "    def __getitem__(self, idx):\n",
                "        \n",
                "        return (self.data.iloc[idx,0], self.data.iloc[idx,1]) # (target, text)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 73,
            "metadata": {},
            "outputs": [],
            "source": [
                "# now we can build our torch dataset \n",
                "train_torch = SpamDataset(train_data)\n",
                "val_torch = SpamDataset(val_data)\n",
                "test_torch = SpamDataset(test_data)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 74,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "(0, 'I not free today i haf 2 pick my parents up tonite...')"
                        ]
                    },
                    "execution_count": 74,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "# check\n",
                "train_torch.__getitem__(2)"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "**Tokenization**\n",
                "\n",
                "Before we build a network, we need to tokenize words from these messages. We will use `torchtext` package, which is a package compatible with pytorch for natural language processing. `torchtext` has an inbuilt tokenizer for English words. To install the package, run\n",
                "```python\n",
                "pip install torchtext \n",
                "```\n",
                "\n",
                "There are two steps in tokenization. 1) build a vocabulary. 2) transform your text based on vocabulary"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 75,
            "metadata": {},
            "outputs": [],
            "source": [
                "#!pip install torchtext"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 76,
            "metadata": {},
            "outputs": [],
            "source": [
                "from torchtext.data.utils import get_tokenizer\n",
                "\n",
                "# build a tokenizer with basic_english\n",
                "tokenizer = get_tokenizer('basic_english')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 77,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "['i', 'am', 'happy']"
                        ]
                    },
                    "execution_count": 77,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "tokenizer('I am happy')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 78,
            "metadata": {},
            "outputs": [],
            "source": [
                "from torchtext.vocab import vocab\n",
                "from collections import Counter\n",
                "\n",
                "# ===== step 1: build vocabulary =====\n",
                "# In pytorch, we need to use the Vocab object to store the vocabulary. The Vocab builds on a Counter object.\n",
                "# Counter object keep tracks of number of occurences of each word\n",
                "# thus you may specify the min_freq to filter out infrequent words\n",
                "counter = Counter() \n",
                "for msg in data['Message']:\n",
                "    counter.update(tokenizer(msg))\n",
                "vocabulary = vocab(counter, min_freq = 3) # filter out all words that appear less than three times\n",
                "\n",
                "# set default index = 0 for words that are not in covabulary\n",
                "vocabulary.set_default_index(0)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 79,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "2961"
                        ]
                    },
                    "execution_count": 79,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "len(vocabulary)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 80,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "100\n1823\n0\n"
                }
            ],
            "source": [
                "# The vocab object maps a word to an idx (an integer)\n",
                "print(vocabulary['my'])\n",
                "print(vocabulary['sun'])\n",
                "print(vocabulary['iertuei']) # something not in vocab will be mapped to default_index = 0"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 81,
            "metadata": {},
            "outputs": [],
            "source": [
                "# define a function that converts a document into tokens (represented by index)\n",
                "def doc_tokenizer(doc):\n",
                "    return torch.tensor([vocabulary[token] for token in tokenizer(doc)], dtype=torch.long)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 82,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "tensor([  61,  307, 1086])"
                        ]
                    },
                    "execution_count": 82,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "doc_tokenizer('I love music')"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Step 2 is to transform documents into tokens based on the vocabulary. Note in real applications where the number of documents is huge, converting all documents before training is very inefficient. As an alternative, we can read in and convert a batch of documents every iteration. This can be easily achieved by defining a `collate_batch` function that will be passed in DataLoader. \n",
                "\n",
                "This basic idea is to read in a batch, apply `collate_batch` and return the processed texts."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 83,
            "metadata": {},
            "outputs": [],
            "source": [
                "# ========= Step 2 ==============\n",
                "# Notice in a corpus, each document can have different size. Thus, we usually pad zeros to the maximum length of document.\n",
                "# Alternatively, you can concat all documents into a long vector \n",
                "# and the starting index of each document is identified in the variable called offsets.\n",
                "\n",
                "def collate_batch(batch):\n",
                "    \n",
                "    target_list, text_list, offsets = [], [], [] \n",
                "    \n",
                "    # loop through all samples in batch\n",
                "    for idx in range(len(batch)):\n",
                "        \n",
                "        _label = batch[idx][0]\n",
                "        _text = batch[idx][1]\n",
                "        \n",
                "        target_list.append( _label )\n",
                "        tokens = doc_tokenizer( _text )\n",
                "        text_list.append(tokens)\n",
                "        \n",
                "        if idx == 0:\n",
                "            offsets.append(0)  # the first document starts from idx 0\n",
                "        else:\n",
                "            offsets.append(offsets[-1] + tokens.size(0)) # the next document starts from (offsets[-1] + tokens.size(0))\n",
                "    \n",
                "    # convert to torch tensor\n",
                "    target_list = torch.tensor(target_list, dtype=torch.int64)\n",
                "    offsets = torch.tensor(offsets)\n",
                "    text_list = torch.cat(text_list) # concat into a long vector\n",
                "    \n",
                "    return target_list, text_list, offsets"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "**Dataloader with customized collating function**\n",
                "\n",
                "We can now build our dataloader by passing in our defined torch dataset and the collate_batch function. Note we also build data loader for validation and test sets. This is because for real datasets, validation and test data can be very large in size. Thus sometimes it may be difficult to test on every sample in validation set. \n",
                "\n",
                "Another reason is that we have `collate_batch` function that process data on the go. If we don't build data loader for validation and test sets, we may need to process them before training or define a different function for that purpose."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 84,
            "metadata": {},
            "outputs": [],
            "source": [
                "torch.manual_seed(0)\n",
                "\n",
                "batchSize = 8\n",
                "train_loader = utils.data.DataLoader(train_torch, batch_size=batchSize, shuffle=True, collate_fn=collate_batch)\n",
                "val_loader = utils.data.DataLoader(val_torch, batch_size=batchSize, shuffle=True, collate_fn=collate_batch)\n",
                "test_loader = utils.data.DataLoader(test_torch, batch_size=batchSize, shuffle=False, collate_fn=collate_batch)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 85,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "(tensor([0, 0, 0, 0, 0, 0, 0, 0]),\n",
                            " tensor([  60,   75,   50,   51,  972,    3,  235,   89,  275,  278, 1439, 1780,\n",
                            "          146, 1781,  146,  537,  254,   50,   51, 1402,   28,  804,  147,    0,\n",
                            "         1022,  972,  175,   61,  401,    8,  295,  157,  151,  237,   31, 2669,\n",
                            "          220,   19,    5,    5,   18,   50,   51,  246, 1814,   93,  483,  581,\n",
                            "            3,  100,  307,   84,  584,  554, 2475,   93,  296,   89,  848,  106,\n",
                            "           93,  317,  306,   89,  306,   93,  296,   89,  323,  797,   80,    0,\n",
                            "            3,  876,   93,  146,  446,   80,    0,  105,  100,  307,   93,  235,\n",
                            "         1414,   32,  146,    0,  477,  582,  166,  102, 1269,   48,   61,   50,\n",
                            "          158,  159,    0,  331,  100,    0,    5, 2675,   61,  211,   31, 1429,\n",
                            "          147, 1588, 1763,   48, 2036,    0, 1056,  401,  881,  157,  435,    5,\n",
                            "          402,  961,  887,  160,   16,    8,    0,   59,  309,  312,    0,   31,\n",
                            "            0, 1395,    5,  184,  106,  551,  566,   31,  161,    5]),\n",
                            " tensor([  0,   9,  19,  56,  67,  76,  85, 115]))"
                        ]
                    },
                    "execution_count": 85,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "# check for the first batch\n",
                "list(train_loader)[0]"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "**Model building**\n",
                "\n",
                "Now we can build our model. There are two choices for embedding layer. One is `nn.Embedding()` and the other is `nn.EmbeddingBag()`. See https://pytorch.org/docs/stable/generated/torch.nn.EmbeddingBag.html. Basically, `nn.EmbeddingBag()` combines `nn.Embedding()` with an aggregation step, either 'sum', 'mean' or 'mode'. \n",
                "\n",
                "*Why you need an aggregation step?* \n",
                "\n",
                "This is because for every word in one document, we have one vector. Thus, each document is a matrix of size $N_w \\times d$, where $N_w$ is the number of words in this document and $d$ is the vector dimension. Usually, we can just take the sum/mean/max of these word vectors in one document and use the resulting vector as representation.\n",
                ""
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 86,
            "metadata": {},
            "outputs": [],
            "source": [
                "# ====== Step 1 ========= \n",
                "class SpamClassifier(nn.Module):\n",
                "    def __init__(self, vocab_size, embed_dim):\n",
                "        super().__init__()\n",
                "        self.embedding = nn.EmbeddingBag(vocab_size, embed_dim, mode='mean') # embedding layer\n",
                "        self.Linear1 = nn.Linear(embed_dim, 1)\n",
                "        self.Dropout = nn.Dropout(p=0.1)\n",
                "    \n",
                "    def forward(self, text, offsets):\n",
                "        # note we need offsets to indicate which document we have\n",
                "        out = self.embedding(text, offsets)\n",
                "        out = self.Dropout(out)\n",
                "        out = self.Linear1(out)\n",
                "        return out\n",
                "        # for the last layer, we don't apply activation because we can use BCEWithLogitsLoss to combine sigmoid with BCELoss\n",
                "        \n",
                "# model initalization\n",
                "embed_dim = 8\n",
                "model = SpamClassifier(len(vocabulary), embed_dim)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 87,
            "metadata": {},
            "outputs": [],
            "source": [
                "# ======= Step 2 ==========\n",
                "loss_fn = torch.nn.BCEWithLogitsLoss()\n",
                "optimizer = torch.optim.Adam(model.parameters(), lr=0.005)"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "For step 3, notice we build data loader for both validation and test sets. It is better to define a function for evaluation."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 88,
            "metadata": {},
            "outputs": [],
            "source": [
                "def evaluate(dataloader):\n",
                "    \n",
                "    y_pred = torch.tensor([]) # store prediction\n",
                "    y_true = torch.tensor([]) # store true label\n",
                "    \n",
                "    model.eval()\n",
                "    with torch.no_grad():\n",
                "        for label, text, offsets in dataloader:\n",
                "            y_pred_batch = model(text, offsets)\n",
                "            \n",
                "            y_pred = torch.cat((y_pred, y_pred_batch.squeeze()))\n",
                "            y_true = torch.cat((y_true, label.squeeze()))\n",
                "            \n",
                "    return y_pred, y_true"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 89,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "Epoch 0: 0.1313 (train), 0.3079 (val), 0.8824 (val acc)\nEpoch 1: 0.0855 (train), 0.2587 (val), 0.8968 (val acc)\nEpoch 2: 0.0300 (train), 0.2490 (val), 0.9031 (val acc)\nEpoch 3: 0.2668 (train), 0.2574 (val), 0.8941 (val acc)\nEpoch 4: 0.7698 (train), 0.2591 (val), 0.8950 (val acc)\nEpoch 5: 0.7047 (train), 0.2283 (val), 0.9111 (val acc)\nEpoch 6: 0.3694 (train), 0.2283 (val), 0.9129 (val acc)\nEpoch 7: 0.5019 (train), 0.2073 (val), 0.9201 (val acc)\nEpoch 8: 0.4560 (train), 0.2304 (val), 0.9111 (val acc)\nEpoch 9: 0.2780 (train), 0.2333 (val), 0.9031 (val acc)\nEpoch 10: 0.0453 (train), 0.2315 (val), 0.9093 (val acc)\nEpoch 11: 0.0934 (train), 0.2432 (val), 0.9048 (val acc)\nEpoch 12: 0.3313 (train), 0.2261 (val), 0.9084 (val acc)\nEpoch 13: 0.2777 (train), 0.2264 (val), 0.9174 (val acc)\nEpoch 14: 0.6968 (train), 0.2428 (val), 0.8977 (val acc)\nEpoch 15: 0.1649 (train), 0.2218 (val), 0.9156 (val acc)\nEpoch 16: 0.0262 (train), 0.2391 (val), 0.9156 (val acc)\nEpoch 17: 0.1703 (train), 0.2429 (val), 0.8932 (val acc)\nEpoch 18: 0.5203 (train), 0.2287 (val), 0.9102 (val acc)\nEpoch 19: 0.2029 (train), 0.2393 (val), 0.9147 (val acc)\nEpoch 20: 0.3855 (train), 0.2333 (val), 0.9111 (val acc)\nEpoch 21: 0.1061 (train), 0.2090 (val), 0.9192 (val acc)\nEpoch 22: 0.1619 (train), 0.2498 (val), 0.9048 (val acc)\nEpoch 23: 0.5449 (train), 0.2635 (val), 0.8959 (val acc)\nEpoch 24: 0.0962 (train), 0.2241 (val), 0.9156 (val acc)\nEpoch 25: 0.1198 (train), 0.2054 (val), 0.9165 (val acc)\nEpoch 26: 0.0217 (train), 0.2245 (val), 0.9129 (val acc)\nEpoch 27: 0.3201 (train), 0.2221 (val), 0.9174 (val acc)\nEpoch 28: 0.2269 (train), 0.2218 (val), 0.9147 (val acc)\nEpoch 29: 0.4205 (train), 0.2017 (val), 0.9156 (val acc)\n"
                }
            ],
            "source": [
                "# ======== Step 3 ==============\n",
                "epochs = 30\n",
                "for epoch in range(epochs):\n",
                "    \n",
                "    for y_train, text, offsets in train_loader:\n",
                "        # zero the parameter gradients\n",
                "        optimizer.zero_grad()\n",
                "\n",
                "        # calulate output and loss \n",
                "        y_pred_train = model(text, offsets)\n",
                "        loss = loss_fn(y_pred_train.squeeze(), y_train.float())\n",
                "\n",
                "        # backprop and take a step\n",
                "        loss.backward()\n",
                "        optimizer.step()\n",
                "    \n",
                "    # evaluate on validation set\n",
                "    y_pred_val, y_val = evaluate(val_loader)\n",
                "    loss_val = loss_fn(y_pred_val.squeeze(), y_val.float())\n",
                "    \n",
                "    # note when making prediction, do add sigmoid activation\n",
                "    pred_label = (torch.sigmoid(y_pred_val) \u003e 0.5).long() # find out the class prediction\n",
                "    acc = (pred_label == y_val).float().sum()/y_val.shape[0]\n",
                "    \n",
                "    model.train() # because when evaluating we change mode to eval mode\n",
                "    \n",
                "    print('Epoch {}: {:.4f} (train), {:.4f} (val), {:.4f} (val acc)'.format(epoch, loss, loss_val, acc))"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 90,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "[[922  32]\n [ 95  65]]\n              precision    recall  f1-score   support\n\n         0.0       0.91      0.97      0.94       954\n         1.0       0.67      0.41      0.51       160\n\n    accuracy                           0.89      1114\n   macro avg       0.79      0.69      0.72      1114\nweighted avg       0.87      0.89      0.87      1114\n\n"
                }
            ],
            "source": [
                "# prediction on test data\n",
                "y_pred_test, y_true_test = evaluate(test_loader)\n",
                "y_pred_test = torch.sigmoid(y_pred_test) \u003e 0.5\n",
                "\n",
                "print(confusion_matrix(y_true_test, y_pred_test))\n",
                "print(classification_report(y_true_test, y_pred_test))"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Going further: More advanced network structure for word embedding (optional)\n",
                "\n",
                "In Task 2, we have seen how we can include an embedding layer in our neural network model. But we simply take the mean of all word vectors in one document and use the resulting vector as representation for this document. This can result in some information loss. Rather, we would like to take all word vectors in one document into account. There are many advanced models that perform this task, such as Recurrent Neural Network (RNN), Long Short Term Memory (LSTM) and so many. Natural language processing is a very exciting area of research.\n",
                "\n",
                "Below we show you an example of using LSTM. But notice, we need to redefine collate batch and evaluate functions. If you want to know more about LSTM. Check this tutorial: https://colah.github.io/posts/2015-08-Understanding-LSTMs/"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 55,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Note in Task 2, we store a batch of documents as a long vector and use offsets to indicate the starting index of each document\n",
                "# Now we need to store the documents as a list of tensors of varying size \n",
                "# And we will pad them to the same lengths using 0\n",
                "\n",
                "from torch.nn.utils.rnn import pad_sequence\n",
                "\n",
                "def collate_batch_advanced(batch):\n",
                "    \n",
                "    target_list, text_list = [], []\n",
                "    \n",
                "    # loop through all samples in batch\n",
                "    for idx in range(len(batch)):\n",
                "        \n",
                "        _label = batch[idx][0]\n",
                "        _text = batch[idx][1]\n",
                "        \n",
                "        target_list.append( _label )\n",
                "        tokens = doc_tokenizer( _text )\n",
                "        text_list.append(tokens)\n",
                "            \n",
                "    # convert to torch tensor\n",
                "    target_list = torch.tensor(target_list, dtype=torch.int64)\n",
                "    \n",
                "    return target_list, text_list\n",
                "\n",
                "\n",
                "# define the evaluate function, notice we need to pad each document with 0\n",
                "def evaluate_adv(dataloader):\n",
                "    \n",
                "    y_pred = torch.tensor([]) # store prediction\n",
                "    y_true = torch.tensor([]) # store true label\n",
                "    \n",
                "    model.eval()\n",
                "    with torch.no_grad():\n",
                "        for label, text in dataloader:\n",
                "            \n",
                "            # we will need to pad the sequences with zero, batch_first means to organize batch size to be the first dim\n",
                "            # and we use 0 to pad them to the same lengths\n",
                "            text = pad_sequence(text, batch_first=True, padding_value=0)\n",
                "            \n",
                "            y_pred_batch = model(text)\n",
                "            \n",
                "            y_pred = torch.cat((y_pred, y_pred_batch.squeeze()))\n",
                "            y_true = torch.cat((y_true, label.squeeze()))\n",
                "            \n",
                "    return y_pred, y_true"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 56,
            "metadata": {},
            "outputs": [],
            "source": [
                "torch.manual_seed(0)\n",
                "\n",
                "batchSize = 64\n",
                "train_loader = utils.data.DataLoader(train_torch, batch_size=batchSize, shuffle=True, collate_fn=collate_batch_advanced)\n",
                "val_loader = utils.data.DataLoader(val_torch, batch_size=batchSize, shuffle=True, collate_fn=collate_batch_advanced)\n",
                "test_loader = utils.data.DataLoader(test_torch, batch_size=batchSize, shuffle=False, collate_fn=collate_batch_advanced)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 57,
            "metadata": {},
            "outputs": [],
            "source": [
                "# ====== Step 1 ========= \n",
                "# The following code is modified from https://towardsdatascience.com/text-classification-with-pytorch-7111dae111a6\n",
                "class SpamAdvClassifier(nn.Module):\n",
                "    def __init__(self, vocab_size, embed_dim, hidden_dim, num_LstmLayer):\n",
                "        super().__init__()\n",
                "        \n",
                "        self.hidden_dim = hidden_dim\n",
                "        self.num_LstmLayer = num_LstmLayer\n",
                "        \n",
                "        # switch to embedding layer: padding_idx = 0 means we treat index=0 as padding and don't train its embedding\n",
                "        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=0) \n",
                "        \n",
                "        # LSTM layer: https://pytorch.org/docs/stable/generated/torch.nn.LSTM.html\n",
                "        self.lstm = nn.LSTM(input_size=embed_dim, hidden_size=hidden_dim, num_layers = num_LstmLayer, batch_first=True) \n",
                "        \n",
                "        self.Linear1 = nn.Linear(hidden_dim, 1)\n",
                "        self.Dropout = nn.Dropout(p=0.1)\n",
                "    \n",
                "    def forward(self, x):\n",
                "        # Hidden and cell state definion\n",
                "        h = torch.zeros((self.num_LstmLayer, x.size(0), self.hidden_dim))\n",
                "        c = torch.zeros((self.num_LstmLayer, x.size(0), self.hidden_dim))\n",
                "        \n",
                "        # Initialization fo hidden and cell states\n",
                "        torch.nn.init.xavier_normal_(h)\n",
                "        torch.nn.init.xavier_normal_(c)\n",
                "        \n",
                "        # embedding layer \n",
                "        out = self.embedding(x)\n",
                "        # lstm layer\n",
                "        out, (hidden, cell) = self.lstm(out, (h,c))\n",
                "        out = self.Dropout(out)\n",
                "        # The last hidden state is taken\n",
                "        out = self.Linear1(out[:,-1,:])\n",
                "        \n",
                "        return out\n",
                "        \n",
                "# model initalization\n",
                "embed_dim = 16\n",
                "hidden_dim = 16\n",
                "num_LstmLayer = 2\n",
                "model = SpamAdvClassifier(len(vocabulary), embed_dim, hidden_dim, num_LstmLayer)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 58,
            "metadata": {},
            "outputs": [],
            "source": [
                "# ======= Step 2 ==========\n",
                "loss_fn = torch.nn.BCEWithLogitsLoss()\n",
                "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "The following code may take some time to run."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 60,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "Epoch 0: 0.5960 (train), 0.3699 (val), 0.8797 (val acc)\nEpoch 1: 0.5203 (train), 0.3674 (val), 0.8797 (val acc)\nEpoch 2: 0.2446 (train), 0.3671 (val), 0.8797 (val acc)\nEpoch 3: 0.4836 (train), 0.3571 (val), 0.8797 (val acc)\nEpoch 4: 0.1948 (train), 0.2268 (val), 0.8797 (val acc)\nEpoch 5: 0.1212 (train), 0.1414 (val), 0.9497 (val acc)\nEpoch 6: 0.0059 (train), 0.1505 (val), 0.9443 (val acc)\nEpoch 7: 0.1620 (train), 0.1370 (val), 0.9596 (val acc)\nEpoch 8: 0.1431 (train), 0.1300 (val), 0.9506 (val acc)\nEpoch 9: 0.0246 (train), 0.1189 (val), 0.9560 (val acc)\nEpoch 10: 0.0545 (train), 0.1038 (val), 0.9704 (val acc)\nEpoch 11: 0.0140 (train), 0.1062 (val), 0.9731 (val acc)\nEpoch 12: 0.2050 (train), 0.1083 (val), 0.9686 (val acc)\nEpoch 13: 0.1420 (train), 0.1338 (val), 0.9623 (val acc)\nEpoch 14: 0.0123 (train), 0.0813 (val), 0.9767 (val acc)\nEpoch 15: 0.0476 (train), 0.0752 (val), 0.9785 (val acc)\nEpoch 16: 0.0150 (train), 0.0785 (val), 0.9776 (val acc)\nEpoch 17: 0.0051 (train), 0.0780 (val), 0.9794 (val acc)\nEpoch 18: 0.0033 (train), 0.0815 (val), 0.9785 (val acc)\nEpoch 19: 0.0192 (train), 0.0813 (val), 0.9794 (val acc)\nEpoch 20: 0.0014 (train), 0.0924 (val), 0.9794 (val acc)\nEpoch 21: 0.0048 (train), 0.0827 (val), 0.9785 (val acc)\nEpoch 22: 0.0045 (train), 0.0835 (val), 0.9785 (val acc)\nEpoch 23: 0.0022 (train), 0.0766 (val), 0.9811 (val acc)\nEpoch 24: 0.0026 (train), 0.0763 (val), 0.9820 (val acc)\nEpoch 25: 0.0016 (train), 0.0682 (val), 0.9820 (val acc)\nEpoch 26: 0.0040 (train), 0.0751 (val), 0.9838 (val acc)\nEpoch 27: 0.0044 (train), 0.0675 (val), 0.9847 (val acc)\nEpoch 28: 0.0019 (train), 0.0686 (val), 0.9829 (val acc)\nEpoch 29: 0.1503 (train), 0.0896 (val), 0.9794 (val acc)\n"
                }
            ],
            "source": [
                "# ======== Step 3 ==============\n",
                "epochs = 30\n",
                "for epoch in range(epochs):\n",
                "    \n",
                "    for y_train, text in train_loader:\n",
                "        # zero the parameter gradients\n",
                "        optimizer.zero_grad()\n",
                "        \n",
                "        # we will need to pad the sequences with zero\n",
                "        text = pad_sequence(text, batch_first=True, padding_value=0)\n",
                "\n",
                "        # calulate output and loss \n",
                "        y_pred_train = model(text)\n",
                "        loss = loss_fn(y_pred_train.squeeze(), y_train.float())\n",
                "\n",
                "        # backprop and take a step\n",
                "        loss.backward()\n",
                "        optimizer.step()\n",
                "    \n",
                "    # evaluate on validation set\n",
                "    y_pred_val, y_val = evaluate_adv(val_loader)\n",
                "    loss_val = loss_fn(y_pred_val.squeeze(), y_val.float())\n",
                "    \n",
                "    # note when making prediction, do add sigmoid activation\n",
                "    pred_label = (torch.sigmoid(y_pred_val) \u003e 0.5).long() # find out the class prediction\n",
                "    acc = (pred_label == y_val).float().sum()/y_val.shape[0]\n",
                "    \n",
                "    model.train() # because when evaluating we change mode to eval mode\n",
                "    \n",
                "    print('Epoch {}: {:.4f} (train), {:.4f} (val), {:.4f} (val acc)'.format(epoch, loss, loss_val, acc))"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 61,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "[[940  14]\n [ 10 150]]\n              precision    recall  f1-score   support\n\n         0.0       0.99      0.99      0.99       954\n         1.0       0.91      0.94      0.93       160\n\n    accuracy                           0.98      1114\n   macro avg       0.95      0.96      0.96      1114\nweighted avg       0.98      0.98      0.98      1114\n\n"
                }
            ],
            "source": [
                "# prediction on test data\n",
                "y_pred_test, y_true_test = evaluate_adv(test_loader)\n",
                "y_pred_test = torch.sigmoid(y_pred_test) \u003e 0.5\n",
                "\n",
                "print(confusion_matrix(y_true_test, y_pred_test))\n",
                "print(classification_report(y_true_test, y_pred_test))"
            ]
        }
    ]
}
